{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "121715f4",
   "metadata": {},
   "source": [
    "# Data Preparation for the New Entity Matching Benckmark\n",
    "\n",
    "Following extensive research and conscientious analysis, we have developed a new benchmark with respect to the entity matching task, specifically tailored for approaches with foundation models. This benchmark encompasses datasets from five diverse domains: restaurant, book, paper, movie, and product. Each domain features three unique datasets.\n",
    "\n",
    "P.S. All the source datasets are obtained from the [Magellan Data Repository](https://sites.google.com/site/anhaidgroup/useful-stuff/the-magellan-data-repository). This notebook contains all the necessary code to construct the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc9e1acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import requests\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "np.random.seed(42)\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c742e0a2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Download Source Datasets to Local\n",
    "\n",
    "The initial step involves downloading all the source datasets from the Magellan repository. Since these datasets are provided in a compressed format, we will decompress them and only retain the decompressed versions locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e89806e6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "urls = {\n",
    "    'restaurant': ['http://pages.cs.wisc.edu/~anhai/data/784_data/restaurants2.tar.gz', \n",
    "                   'http://pages.cs.wisc.edu/~anhai/data/784_data/restaurants4.tar.gz', \n",
    "                   'http://pages.cs.wisc.edu/~anhai/data/corleone_data/restaurants.tar.gz'],\n",
    "    'book': ['http://pages.cs.wisc.edu/~anhai/data/784_data/books2.tar.gz', \n",
    "             'http://pages.cs.wisc.edu/~anhai/data/784_data/books3.tar.gz', \n",
    "             'http://pages.cs.wisc.edu/~anhai/data/784_data/books5.tar.gz'],\n",
    "    'paper': ['http://pages.cs.wisc.edu/~anhai/data/corleone_data/citations.tar.gz', \n",
    "              'http://pages.cs.wisc.edu/~anhai/data/wisc_em_benchmark/839_spring19/CompVision/csv/CompVision.tar.gz', \n",
    "              'http://pages.cs.wisc.edu/~anhai/data/wisc_em_benchmark/839_spring19/AcadPapers/csv/AcadPapers.tar.gz'],\n",
    "    'movie': ['http://pages.cs.wisc.edu/~anhai/data/784_data/movies1.tar.gz', \n",
    "              'http://pages.cs.wisc.edu/~anhai/data/784_data/movies4.tar.gz', \n",
    "              'http://pages.cs.wisc.edu/~anhai/data/784_data/movies5.tar.gz'],\n",
    "    'product': ['http://pages.cs.wisc.edu/~anhai/data/784_data/cosmetics.tar.gz', \n",
    "                'http://pages.cs.wisc.edu/~anhai/data/784_data/baby_products.tar.gz', \n",
    "                'http://pages.cs.wisc.edu/~anhai/data/corleone_data/products.tar.gz']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fac9ef6d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def download_and_extract(url, extract_dir='.'):\n",
    "    \"\"\"\n",
    "    Download a .tar.gz file from a specified URL and extract its contents.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(\"temp_file.tar.gz\", 'wb') as file:\n",
    "            file.write(response.raw.read())\n",
    "        \n",
    "        with tarfile.open(\"temp_file.tar.gz\", \"r:gz\") as tar_ref:\n",
    "            tar_ref.extractall(path=extract_dir)\n",
    "        \n",
    "        os.remove(\"temp_file.tar.gz\")\n",
    "    else:\n",
    "        print(f\"Error downloading the file: HTTP {response.status_code}\")\n",
    "        \n",
    "        \n",
    "# [download_and_extract(url, os.path.join('dirty', domain)) for domain in urls for url in urls[domain]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7a0eb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edcd6891",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Restaurant Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2316ff49",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "src_restaurant_dir_path = 'dirty/restaurant'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96526834",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fodors & Zagats\n",
    "The dataset collector did not specify Table A and Table B. Therefore, in alignment with the practices of existing benchmarks, we have named the final dataset 'Fodors_Zagats.' A separate file has been designated to store the matching pairs annotated by humans.\n",
    "\n",
    "Metadata of the source data: The Fodor dataset comprises 533 samples, while the Zagat dataset contains 331 samples. Human annotators have labeled 112 pairs as gold-standard matches. Additionally, there are 82 pairs that exhibit an exact match at the string level in the 'name' column. \n",
    "\n",
    "The following are details to generate postive candidates (matches) and negative candidates (non-matches):\n",
    "\n",
    "**postive candidates** : \n",
    "1. The nameA contains nameB, vice verse; OR\n",
    "2. The addressA contains addressB, vice verse; OR\n",
    "3. The phoneA contains phoneB, vice verse; OR\n",
    "4. The levenshtein similarity between nameA and nameB is larger than 0.9.\n",
    "\n",
    "**negative candidates** :\n",
    "1. The idA and idB is distinct from the positive candidates; AND\n",
    "2. The idA and idB is distinct from the golden candidates.\n",
    "3. 1000 pairs are randonly selected afterward.\n",
    "\n",
    "The ['Name', 'Address', 'Phone'] columns are used for the later manual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86626cf1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n",
      "112\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'restaurants'\n",
    "tableA_path = os.path.join(src_restaurant_dir_path, dataset_path, 'fodors.csv')\n",
    "tableB_path = os.path.join(src_restaurant_dir_path, dataset_path, 'zagats.csv')\n",
    "\n",
    "tableA = pd.read_csv(tableA_path)\n",
    "tableB = pd.read_csv(tableB_path)\n",
    "\n",
    "tableA.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "tableB.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "match_path = os.path.join(src_restaurant_dir_path, dataset_path, 'matches_fodors_zagats.csv')\n",
    "gold_match = pd.read_csv(match_path)\n",
    "\n",
    "print(len(set(tableA.name).intersection(set(tableB.name))))\n",
    "print(len(gold_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99d3893a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old columns are: \n",
      " Index(['id', 'name', 'addr', 'city', 'phone', 'type', 'class'], dtype='object') \n",
      " Index(['id', 'name', 'addr', 'city', 'phone', 'type', 'class'], dtype='object')\n",
      "\n",
      " The new columns are: \n",
      " Index(['ID', 'Name', 'Address', 'City', 'Phone', 'type'], dtype='object') \n",
      " Index(['ID', 'Name', 'Address', 'City', 'Phone', 'type'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# clean up\n",
    "print('The old columns are: \\n', tableA.columns, '\\n', tableB.columns)\n",
    "\n",
    "new_tableA = tableA.drop(columns=['class']).rename(columns={'id': 'ID', 'name': 'Name', 'addr': 'Address', \n",
    "                                                            'city': 'City', 'phone': 'Phone'})\n",
    "new_tableB = tableB.drop(columns=['class']).rename(columns={'id': 'ID', 'name': 'Name', 'addr': 'Address', \n",
    "                                                            'city': 'City', 'phone': 'Phone'})\n",
    "\n",
    "print('\\n The new columns are: \\n', new_tableA.columns, '\\n', new_tableB.columns)\n",
    "\n",
    "\n",
    "# store new_tableA and new_tableB in the cleaned directory\n",
    "save_dir = 'cleaned/restaurant/Fodors-Zagats'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "new_tableA.to_csv(os.path.join(save_dir, 'tableA.csv'), index=False)\n",
    "new_tableB.to_csv(os.path.join(save_dir, 'tableB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e077ffa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp_tableA = new_tableA[['ID', 'Name', 'Address', 'Phone']].copy()\n",
    "temp_tableB = new_tableB[['ID', 'Name', 'Address', 'Phone']].copy()\n",
    "\n",
    "temp_tableA['_Phone'] = temp_tableA['Phone'].str.replace('/', '-')\n",
    "temp_tableA['_Address'] = temp_tableA['Address'].apply(lambda x: 'This is not an address' if x == '4' else x)\n",
    "\n",
    "connection = duckdb.connect(database=':memory:', read_only=False)\n",
    "connection.register('A', temp_tableA)\n",
    "connection.register('B', temp_tableB)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    ta.ID AS ID_A,\n",
    "    tb.ID AS ID_B,\n",
    "    ta.Name as Name_A,\n",
    "    tb.Name as Name_B,\n",
    "    ta.Address as Address_A,\n",
    "    tb.Address as Address_B,\n",
    "    ta.Phone as Phone_A,\n",
    "    tb.Phone as Phone_B\n",
    "    \n",
    "FROM\n",
    "    A ta\n",
    "CROSS JOIN\n",
    "    B tb\n",
    "WHERE\n",
    "    (ta.Name LIKE '%' || tb.Name || '%' OR tb.Name LIKE '%' || ta.Name || '%') OR\n",
    "    (ta._Address LIKE '%' || tb.Address || '%' OR tb.Address LIKE '%' || ta._Address || '%') OR\n",
    "    (ta._Phone LIKE '%' || tb.Phone || '%' OR tb.Phone LIKE '%' || ta._Phone || '%') OR\n",
    "    (levenshtein(ta.Name, tb.Name) / GREATEST(LENGTH(ta.Name), LENGTH(tb.Name)) < 0.1)\n",
    "\"\"\"\n",
    "\n",
    "result = connection.execute(query).fetchdf()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff46775b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## generate pos_candidates & neg_candidates, and store them in the cleaned directory\n",
    "pos_candidates = pd.DataFrame({\n",
    "    'ID_A': result['ID_A'], 'ID_B': result['ID_B'],\n",
    "    'Name_A': result['Name_A'], 'Name_B': result['Name_B'],\n",
    "    'Address_A': result['Address_A'], 'Address_B': result['Address_B'],\n",
    "    'Phone_A': result['Phone_A'], 'Phone_B': result['Phone_B'],\n",
    "})\n",
    "\n",
    "temp_tableA['temp_key'] = 1\n",
    "temp_tableB['temp_key'] = 1\n",
    "all_id_pairs = pd.merge(temp_tableA[['ID', 'temp_key']], temp_tableB[['ID', 'temp_key']], on='temp_key', suffixes=('_A', '_B'))\n",
    "all_id_pairs = all_id_pairs.drop('temp_key', axis=1)\n",
    "\n",
    "\n",
    "gold_match = gold_match.rename(columns={'fodors_id': 'ID_A', 'zagats_id': 'ID_B'})\n",
    "exclude_pairs = pd.concat([gold_match, pos_candidates[['ID_A', 'ID_B']]], axis=0).drop_duplicates()\n",
    "neg_candidates = pd.concat([all_id_pairs, exclude_pairs], axis=0).drop_duplicates(keep=False)\n",
    "neg_candidates = neg_candidates.sample(n=1000, random_state=SEED)\n",
    "\n",
    "neg_candidates = neg_candidates.merge(temp_tableA[['ID', 'Name', 'Address', 'Phone']], \n",
    "                                      left_on='ID_A', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates.merge(temp_tableB[['ID', 'Name', 'Address', 'Phone']], \n",
    "                                      left_on='ID_B', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates[list(pos_candidates.columns)]\n",
    "\n",
    "pos_candidates.to_csv(os.path.join(save_dir, 'pos_candidates.csv'), index=False)\n",
    "neg_candidates.to_csv(os.path.join(save_dir, 'neg_candidates.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c044b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77d6334a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Restaurant 2\n",
    "\n",
    "The dataset consist of tableA from Zomato and tableB from Yelp.\n",
    "\n",
    "Metadata of the source data: The Zomato dataset comprises 6960 samples, while the Yelp dataset contains 3897 samples. Human annotators have labeled 90 pairs as gold-standard matches. Additionally, there are 1042 pairs that exhibit an exact match at the string level in the 'name' column.\n",
    "\n",
    "\n",
    "The following are details to generate postive candidates (matches) and negative candidates (non-matches):\n",
    "\n",
    "**postive candidates** : \n",
    "1. The nameA contains nameB, vice verse; AND\n",
    "2. The zipcodeA and zipcodeB are the same.\n",
    "\n",
    "**negative candidates** :\n",
    "1. The idA and idB is distinct from the positive candidates; AND\n",
    "2. The idA and idB is distinct from the golden candidates.\n",
    "3. 1000 pairs are randonly selected afterward.\n",
    "\n",
    "The ['Name', 'Zipcode', 'Address'] columns are used for the later manual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e85d4d3",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1042\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'restaurants2/csv_files'\n",
    "tableA_path = os.path.join(src_restaurant_dir_path, dataset_path, 'zomato.csv')\n",
    "tableB_path = os.path.join(src_restaurant_dir_path, dataset_path, 'yelp.csv')\n",
    "\n",
    "tableA = pd.read_csv(tableA_path)\n",
    "tableB = pd.read_csv(tableB_path)\n",
    "\n",
    "tableA.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "tableB.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "match_path = os.path.join(src_restaurant_dir_path, dataset_path, 'labeled_data.csv')\n",
    "gold_match = pd.read_csv(match_path, skiprows=5)\n",
    "\n",
    "print(len(set(tableA.name).intersection(set(tableB.name))))\n",
    "print(gold_match.gold.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0297a5df",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old columns are: \n",
      " Index(['ID', 'name', 'votes', 'rating', 'phone', 'address', 'zip', 'cuisine',\n",
      "       'reviewcount'],\n",
      "      dtype='object') \n",
      " Index(['ID', 'name', 'votes', 'rating', 'phone', 'address', 'zip', 'cuisine'], dtype='object')\n",
      "\n",
      " The new columns are: \n",
      " Index(['ID', 'Name', 'Votes', 'Rating', 'Phone', 'Address', 'Zipcode',\n",
      "       'Cuisine'],\n",
      "      dtype='object') \n",
      " Index(['ID', 'Name', 'Votes', 'Rating', 'Phone', 'Address', 'Zipcode',\n",
      "       'Cuisine'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# clean up\n",
    "print('The old columns are: \\n', tableA.columns, '\\n', tableB.columns)\n",
    "\n",
    "tableA['cuisine'] = tableA['cuisine'].apply(lambda x: x.replace(',', ', ') if not pd.isnull(x) else x)\n",
    "\n",
    "new_tableA = tableA.drop(columns=['reviewcount']).rename(columns={'name': 'Name', 'votes':'Votes', \n",
    "                                                              'rating': 'Rating', 'phone': 'Phone', \n",
    "                                                              'address': 'Address', 'zip': 'Zipcode', \n",
    "                                                              'cuisine' : 'Cuisine'})\n",
    "new_tableB = tableB.rename(columns={'name': 'Name', 'votes':'Votes', \n",
    "                                         'rating': 'Rating', 'phone': 'Phone', \n",
    "                                         'address': 'Address', 'zip': 'Zipcode', \n",
    "                                         'cuisine' : 'Cuisine'})\n",
    "\n",
    "print('\\n The new columns are: \\n', new_tableA.columns, '\\n', new_tableB.columns)\n",
    "\n",
    "# store new_tableA and new_tableB in the cleaned directory\n",
    "save_dir = 'cleaned/restaurant/Zomato-Yelp'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "new_tableA.to_csv(os.path.join(save_dir, 'tableA.csv'), index=False)\n",
    "new_tableB.to_csv(os.path.join(save_dir, 'tableB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0f2757f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp_tableA = new_tableA[['ID', 'Name', 'Zipcode', 'Address']].copy()\n",
    "temp_tableB = new_tableB[['ID', 'Name', 'Zipcode', 'Address']].copy()\n",
    "\n",
    "connection = duckdb.connect(database=':memory:', read_only=False)\n",
    "connection.register('A', temp_tableA)\n",
    "connection.register('B', temp_tableB)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    ta.ID AS ID_A,\n",
    "    tb.ID AS ID_B,\n",
    "    ta.Name AS Name_A,\n",
    "    tb.Name AS Name_B,\n",
    "    ta.Zipcode As Zipcode_A,\n",
    "    tb.Zipcode As Zipcode_B,\n",
    "    ta.Address AS Address_A,\n",
    "    tb.Address AS Address_B,\n",
    "    \n",
    "    \n",
    "FROM\n",
    "    A ta\n",
    "CROSS JOIN\n",
    "    B tb\n",
    "WHERE\n",
    "    (LOWER(ta.Name) LIKE '%' || LOWER(tb.Name) || '%' OR LOWER(tb.Name) LIKE '%' || LOWER(ta.Name) || '%') AND\n",
    "    (ta.Zipcode == tb.Zipcode)\n",
    "\"\"\"\n",
    "\n",
    "result = connection.execute(query).fetchdf()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de9e4f0a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## generate pos_candidates & neg_candidates, and store them in the cleaned directory\n",
    "pos_candidates = pd.DataFrame({\n",
    "    'ID_A': result['ID_A'], 'ID_B': result['ID_B'],\n",
    "    'Name_A': result['Name_A'], 'Name_B': result['Name_B'],\n",
    "    'Zipcode_A': result['Zipcode_A'], 'Zipcode_B': result['Zipcode_B'],\n",
    "    'Address_A': result['Address_A'], 'Address_B': result['Address_B'],\n",
    "})\n",
    "\n",
    "temp_tableA['temp_key'] = 1\n",
    "temp_tableB['temp_key'] = 1\n",
    "all_id_pairs = pd.merge(temp_tableA[['ID', 'temp_key']], temp_tableB[['ID', 'temp_key']], on='temp_key', suffixes=('_A', '_B'))\n",
    "all_id_pairs = all_id_pairs.drop('temp_key', axis=1)\n",
    "\n",
    "\n",
    "gold_match = gold_match.rename(columns={'ltable.id': 'ID_A', 'rtable.id': 'ID_B'})\n",
    "exclude_pairs = pd.concat([gold_match[['ID_A', 'ID_B']], pos_candidates[['ID_A', 'ID_B']]], axis=0).drop_duplicates()\n",
    "neg_candidates = pd.concat([all_id_pairs, exclude_pairs], axis=0).drop_duplicates(keep=False)\n",
    "neg_candidates = neg_candidates.sample(n=1000, random_state=SEED)\n",
    "\n",
    "neg_candidates = neg_candidates.merge(temp_tableA[['ID', 'Name', 'Zipcode', 'Address']], \n",
    "                                      left_on='ID_A', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates.merge(temp_tableB[['ID', 'Name', 'Zipcode', 'Address']], \n",
    "                                      left_on='ID_B', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates[list(pos_candidates.columns)]\n",
    "\n",
    "pos_candidates.to_csv(os.path.join(save_dir, 'pos_candidates.csv'), index=False)\n",
    "neg_candidates.to_csv(os.path.join(save_dir, 'neg_candidates.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ea194",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47b9991d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Restaurants 4\n",
    "\n",
    "The dataset consist of tableA from Yellow Pages and tableB from Yelp.\n",
    "\n",
    "Metadata of the source data: The Yellow Pages dataset comprises 11840 samples, while the Yelp dataset contains 5223 samples. Human annotators have labeled 130 pairs as gold-standard matches. Additionally, there are 575 pairs that exhibit an exact match at the string level in the 'name' column.\n",
    "\n",
    "\n",
    "The following are details to generate postive candidates (matches) and negative candidates (non-matches):\n",
    "\n",
    "**postive candidates** : \n",
    "1. The nameA contains nameB, vice verse; AND\n",
    "2. The zipcodeA and zipcodeB are the same.\n",
    "3. 1100 pairs are randonly selected afterward.\n",
    "\n",
    "**negative candidates** :\n",
    "1. The idA and idB is distinct from the positive candidates; AND\n",
    "2. The idA and idB is distinct from the golden candidates.\n",
    "3. 1000 pairs are randonly selected afterward.\n",
    "\n",
    "The ['Name', 'Zipcode', 'Address'] columns are used for the later manual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5104505",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575\n",
      "130\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'restaurants4/csv_files'\n",
    "tableA_path = os.path.join(src_restaurant_dir_path, dataset_path, 'yellow_pages.csv')\n",
    "tableB_path = os.path.join(src_restaurant_dir_path, dataset_path, 'yelp.csv')\n",
    "\n",
    "tableA = pd.read_csv(tableA_path)\n",
    "tableB = pd.read_csv(tableB_path)\n",
    "\n",
    "tableA.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "tableB.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "match_path = os.path.join(src_restaurant_dir_path, dataset_path, 'labeled_data.csv')\n",
    "gold_match = pd.read_csv(match_path, skiprows=5)\n",
    "\n",
    "print(len(set(tableA.name).intersection(set(tableB.name))))\n",
    "print(gold_match.gold.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20462d30",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old columns are: \n",
      " Index(['id', 'name', 'address', 'city', 'state', 'zipcode', 'phone', 'website',\n",
      "       'Unnamed: 8'],\n",
      "      dtype='object') \n",
      " Index(['id', 'name', 'address', 'city', 'state', 'zipcode', 'phone'], dtype='object')\n",
      "\n",
      " The new columns are: \n",
      " Index(['ID', 'Name', 'Address', 'City', 'State', 'Zipcode', 'Phone'], dtype='object') \n",
      " Index(['ID', 'Name', 'Address', 'City', 'State', 'Zipcode', 'Phone'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# clean up\n",
    "print('The old columns are: \\n', tableA.columns, '\\n', tableB.columns)\n",
    "\n",
    "new_tableA = tableA.drop(columns=['website', 'Unnamed: 8'])\\\n",
    "                   .rename(columns={'id': 'ID', 'name': 'Name', 'address': 'Address', 'city': 'City', \n",
    "                                    'state': 'State', 'zipcode': 'Zipcode', 'phone': 'Phone'})\n",
    "new_tableB = tableB.drop(columns=[])\\\n",
    "                   .rename(columns={'id': 'ID', 'name': 'Name', 'address': 'Address', 'city': 'City', \n",
    "                                    'state': 'State', 'zipcode': 'Zipcode', 'phone': 'Phone'})\n",
    "\n",
    "print('\\n The new columns are: \\n', new_tableA.columns, '\\n', new_tableB.columns)\n",
    "\n",
    "# store new_tableA and new_tableB in the cleaned directory\n",
    "save_dir = 'cleaned/restaurant/Yellow_Pages-Yelp'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "new_tableA.to_csv(os.path.join(save_dir, 'tableA.csv'), index=False)\n",
    "new_tableB.to_csv(os.path.join(save_dir, 'tableB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec1a4bae",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp_tableA = new_tableA[['ID', 'Name', 'Zipcode', 'Address']].copy()\n",
    "temp_tableB = new_tableB[['ID', 'Name', 'Zipcode', 'Address']].copy()\n",
    "\n",
    "connection = duckdb.connect(database=':memory:', read_only=False)\n",
    "connection.register('A', temp_tableA)\n",
    "connection.register('B', temp_tableB)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    ta.ID AS ID_A,\n",
    "    tb.ID AS ID_B,\n",
    "    ta.Name AS Name_A,\n",
    "    tb.Name AS Name_B,\n",
    "    ta.Zipcode As Zipcode_A,\n",
    "    tb.Zipcode As Zipcode_B,\n",
    "    ta.Address AS Address_A,\n",
    "    tb.Address AS Address_B,\n",
    "    \n",
    "FROM\n",
    "    A ta\n",
    "CROSS JOIN\n",
    "    B tb\n",
    "WHERE\n",
    "    (LOWER(ta.Name) LIKE '%' || LOWER(tb.Name) || '%' OR LOWER(tb.Name) LIKE '%' || LOWER(ta.Name) || '%') AND\n",
    "    (ta.Zipcode == tb.Zipcode)\n",
    "\"\"\"\n",
    "\n",
    "result = connection.execute(query).fetchdf()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52351ad8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## generate pos_candidates & neg_candidates, and store them in the cleaned directory\n",
    "pos_candidates = pd.DataFrame({\n",
    "    'ID_A': result['ID_A'], 'ID_B': result['ID_B'],\n",
    "    'Name_A': result['Name_A'], 'Name_B': result['Name_B'],\n",
    "    'Zipcode_A': result['Zipcode_A'], 'Zipcode_B': result['Zipcode_B'],\n",
    "    'Address_A': result['Address_A'], 'Address_B': result['Address_B'],\n",
    "})\n",
    "\n",
    "temp_tableA['temp_key'] = 1\n",
    "temp_tableB['temp_key'] = 1\n",
    "all_id_pairs = pd.merge(temp_tableA[['ID', 'temp_key']], temp_tableB[['ID', 'temp_key']], on='temp_key', suffixes=('_A', '_B'))\n",
    "all_id_pairs = all_id_pairs.drop('temp_key', axis=1)\n",
    "\n",
    "gold_match = gold_match.rename(columns={'ltable.id': 'ID_A', 'rtable.id': 'ID_B'})\n",
    "exclude_pairs = pd.concat([gold_match[['ID_A', 'ID_B']], pos_candidates[['ID_A', 'ID_B']]], axis=0).drop_duplicates()\n",
    "neg_candidates = pd.concat([all_id_pairs, exclude_pairs], axis=0).drop_duplicates(keep=False)\n",
    "\n",
    "pos_candidates = pos_candidates.sample(n=1100, random_state=SEED)\n",
    "\n",
    "neg_candidates = neg_candidates.sample(n=1000, random_state=SEED)\n",
    "neg_candidates = neg_candidates.merge(temp_tableA[['ID', 'Name', 'Zipcode', 'Address']], \n",
    "                                      left_on='ID_A', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates.merge(temp_tableB[['ID', 'Name', 'Zipcode', 'Address']], \n",
    "                                      left_on='ID_B', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates[list(pos_candidates.columns)]\n",
    "\n",
    "pos_candidates.to_csv(os.path.join(save_dir, 'pos_candidates.csv'), index=False)\n",
    "neg_candidates.to_csv(os.path.join(save_dir, 'neg_candidates.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37fe05",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91889ac6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Book Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bf800fa",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "src_book_dir_path = 'dirty/book'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a71d5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Books 2\n",
    "\n",
    "The dataset consist of tableA from Goodreads and tableB from Barnes and Noble.\n",
    "\n",
    "Metadata of the source data: The Goodreads dataset comprises 3967 samples, while the Barnes and Noble dataset contains 3701 samples. Human annotators have labeled 92 pairs as gold-standard matches. Additionally, there are 676 pairs that exhibit an exact match at the string level in the 'title' column.\n",
    "\n",
    "\n",
    "The following are details to generate postive candidates (matches) and negative candidates (non-matches):\n",
    "\n",
    "**postive candidates** : \n",
    "1. The isbnA and isbnB are the same (when existing); OR\n",
    "2. (The titleA and titleB are the same) AND (The titleA is not Autobiography and An Autobiography).\n",
    "3. 1100 pairs are randonly selected afterward.\n",
    "\n",
    "**negative candidates** :\n",
    "1. The idA and idB is distinct from the positive candidates; AND\n",
    "2. The idA and idB is distinct from the golden candidates.\n",
    "3. 1000 pairs are randonly selected afterward.\n",
    "\n",
    "The ['Title', 'ISBN13'] columns are used for the later manual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e1dee67",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676\n",
      "92\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'books2/csv_files'\n",
    "tableA_path = os.path.join(src_book_dir_path, dataset_path, 'goodreads.csv')\n",
    "tableB_path = os.path.join(src_book_dir_path, dataset_path, 'barnes_and_noble.csv')\n",
    "\n",
    "tableA = pd.read_csv(tableA_path)\n",
    "tableB = pd.read_csv(tableB_path, encoding='latin1')\n",
    "\n",
    "tableA.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "tableB.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "match_path = os.path.join(src_book_dir_path, dataset_path, 'labeled_data.csv')\n",
    "gold_match = pd.read_csv(match_path, skiprows=5)\n",
    "\n",
    "print(len(set(tableA.Title).intersection(set(tableB.Title))))\n",
    "print(gold_match.match_label.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e48d350",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old columns are: \n",
      " Index(['ID', 'Title', 'Description', 'ISBN', 'ISBN13', 'PageCount',\n",
      "       'FirstAuthor', 'SecondAuthor', 'ThirdAuthor', 'Rating',\n",
      "       'NumberofRatings', 'NumberofReviews', 'Publisher', 'PublishDate',\n",
      "       'Format', 'Language', 'FileName'],\n",
      "      dtype='object') \n",
      " Index(['ID', 'Title', 'Author1', 'Author2', 'Author3', 'Publisher', 'ISBN13',\n",
      "       'PublicationDate', 'Pages', 'Productdimensions', 'Salesrank',\n",
      "       'Ratingscount', 'Ratingvalue', 'Paperbackprice', 'Hardcoverprice',\n",
      "       'Nookbookprice', 'Audiobookprice'],\n",
      "      dtype='object')\n",
      "\n",
      " The new columns are: \n",
      " Index(['ID', 'Title', 'Authors', 'ISBN13', 'Pages', 'Rating', 'Ratings_Count',\n",
      "       'Publisher', 'Publish_Date'],\n",
      "      dtype='object') \n",
      " Index(['ID', 'Title', 'Authors', 'ISBN13', 'Pages', 'Rating', 'Ratings_Count',\n",
      "       'Publisher', 'Publish_Date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# clean up\n",
    "print('The old columns are: \\n', tableA.columns, '\\n', tableB.columns)\n",
    "\n",
    "tableA['Authors'] = tableA.apply(lambda x: ', '.join([x[col] for col in ['FirstAuthor', 'SecondAuthor', 'ThirdAuthor'] \n",
    "                                                if pd.notnull(x[col])]), axis=1)\n",
    "tableB['Authors'] = tableB.apply(lambda x: ', '.join([x[col] for col in ['Author1', 'Author2', 'Author3'] \n",
    "                                                if pd.notnull(x[col])]), axis=1)\n",
    "\n",
    "new_tableA = tableA.drop(columns=['Description', 'ISBN', 'FirstAuthor', 'SecondAuthor', 'ThirdAuthor', \n",
    "                                  'NumberofReviews', 'Format', 'Language', 'FileName'])\\\n",
    "                   .rename(columns={'PageCount': 'Pages', 'NumberofRatings': 'Ratings_Count', \n",
    "                                    'PublishDate': 'Publish_Date'})\n",
    "new_tableB = tableB.drop(columns=['Author1', 'Author2', 'Author3', 'Productdimensions', 'Salesrank', \n",
    "                                  'Paperbackprice', 'Hardcoverprice', 'Nookbookprice', 'Audiobookprice'])\\\n",
    "                   .rename(columns={'Ratingscount': 'Ratings_Count', 'Ratingvalue': 'Rating',  \n",
    "                                    'PublicationDate': 'Publish_Date'})\n",
    "new_tableA = new_tableA[['ID', 'Title', 'Authors', 'ISBN13', 'Pages', 'Rating', 'Ratings_Count', 'Publisher', 'Publish_Date']]\n",
    "new_tableB = new_tableB[['ID', 'Title', 'Authors', 'ISBN13', 'Pages', 'Rating', 'Ratings_Count', 'Publisher', 'Publish_Date']]\n",
    "\n",
    "print('\\n The new columns are: \\n', new_tableA.columns, '\\n', new_tableB.columns)\n",
    "\n",
    "# store new_tableA and new_tableB in the cleaned directory\n",
    "save_dir = 'cleaned/book/Goodreads-Barnes_and_Noble'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "new_tableA.to_csv(os.path.join(save_dir, 'tableA.csv'), index=False)\n",
    "new_tableB.to_csv(os.path.join(save_dir, 'tableB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1efeb5d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp_tableA = new_tableA[['ID', 'Title', 'ISBN13']].copy()\n",
    "temp_tableB = new_tableB[['ID', 'Title', 'ISBN13']].copy()\n",
    "\n",
    "connection = duckdb.connect(database=':memory:', read_only=False)\n",
    "connection.register('A', temp_tableA)\n",
    "connection.register('B', temp_tableB)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    ta.ID AS ID_A,\n",
    "    tb.ID AS ID_B,\n",
    "    ta.Title AS Title_A,\n",
    "    tb.Title AS Title_B,\n",
    "    ta.ISBN13 As ISBN13_A,\n",
    "    tb.ISBN13 As ISBN13_B\n",
    "    \n",
    "FROM\n",
    "    A ta\n",
    "CROSS JOIN\n",
    "    B tb\n",
    "WHERE (ta.ISBN13 = tb.ISBN13) OR \n",
    "    ((ta.ISBN13 IS NULL OR tb.ISBN13 IS NULL) AND (ta.Title == tb.Title) AND (ta.Title != 'Autobiography') AND\n",
    "    ((ta.Title != 'An Autobiography')))\n",
    "\"\"\"\n",
    "\n",
    "result = connection.execute(query).fetchdf()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c428c371",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## generate pos_candidates & neg_candidates, and store them in the cleaned directory\n",
    "pos_candidates = pd.DataFrame({\n",
    "    'ID_A': result['ID_A'], 'ID_B': result['ID_B'],\n",
    "    'Title_A': result['Title_A'], 'Title_B': result['Title_B'],\n",
    "    'ISBN13_A': result['ISBN13_A'], 'ISBN13_B': result['ISBN13_B'],\n",
    "})\n",
    "\n",
    "temp_tableA['temp_key'] = 1\n",
    "temp_tableB['temp_key'] = 1\n",
    "all_id_pairs = pd.merge(temp_tableA[['ID', 'temp_key']], temp_tableB[['ID', 'temp_key']], on='temp_key', suffixes=('_A', '_B'))\n",
    "all_id_pairs = all_id_pairs.drop('temp_key', axis=1)\n",
    "\n",
    "gold_match = gold_match.rename(columns={'ltable.ID': 'ID_A', 'rtable.ID': 'ID_B'})\n",
    "exclude_pairs = pd.concat([gold_match[['ID_A', 'ID_B']], pos_candidates[['ID_A', 'ID_B']]], axis=0).drop_duplicates()\n",
    "neg_candidates = pd.concat([all_id_pairs, exclude_pairs], axis=0).drop_duplicates(keep=False)\n",
    "\n",
    "neg_candidates = neg_candidates.sample(n=1000, random_state=SEED)\n",
    "neg_candidates = neg_candidates.merge(temp_tableA[['ID', 'Title', 'ISBN13']], \n",
    "                                      left_on='ID_A', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates.merge(temp_tableB[['ID', 'Title', 'ISBN13']], \n",
    "                                      left_on='ID_B', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates[list(pos_candidates.columns)]\n",
    "\n",
    "pos_candidates.to_csv(os.path.join(save_dir, 'pos_candidates.csv'), index=False)\n",
    "neg_candidates.to_csv(os.path.join(save_dir, 'neg_candidates.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca56a9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2d27fc8",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Books 3\n",
    "\n",
    "The dataset consist of tableA from Barnes and Noble and tableB from Half.\n",
    "\n",
    "Metadata of the source data: The Barnes and Noble dataset comprises 3022 samples, while the Half dataset contains 3099 samples. Human annotators have labeled 327 pairs as gold-standard matches. Additionally, there are 357 pairs that exhibit an exact match at the string level in the 'title' column.\n",
    "\n",
    "\n",
    "The following are details to generate postive candidates (matches) and negative candidates (non-matches):\n",
    "\n",
    "**postive candidates** : \n",
    "1. The isbnA and isbnB are the same (when existing); OR\n",
    "2. The titleA and titleB are the same.\n",
    "\n",
    "**negative candidates** :\n",
    "1. The idA and idB is distinct from the positive candidates; AND\n",
    "2. The idA and idB is distinct from the golden candidates.\n",
    "3. 1000 pairs are randonly selected afterward.\n",
    "\n",
    "The ['Title', 'ISBN13'] columns are used for the later manual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "05fb81c0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357\n",
      "327\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'books3/csv_files'\n",
    "tableA_path = os.path.join(src_book_dir_path, dataset_path, 'barnes_and_noble.csv')\n",
    "tableB_path = os.path.join(src_book_dir_path, dataset_path, 'half.csv')\n",
    "\n",
    "tableA = pd.read_csv(tableA_path)\n",
    "tableB = pd.read_csv(tableB_path)\n",
    "\n",
    "tableA.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "tableB.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "match_path = os.path.join(src_book_dir_path, dataset_path, 'labeled_data.csv')\n",
    "gold_match = pd.read_csv(match_path, skiprows=5)\n",
    "\n",
    "print(len(set(tableA.Title).intersection(set(tableB.Title))))\n",
    "print(gold_match.gold.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d9bf331e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old columns are: \n",
      " Index(['ID', 'Title', 'Price', 'Author', 'ISBN13', 'Publisher',\n",
      "       'Publication_Date', 'Pages', 'Dimensions'],\n",
      "      dtype='object') \n",
      " Index(['ID', 'Title', 'UsedPrice', 'NewPrice', 'Author', 'ISBN10', 'ISBN13',\n",
      "       'Publisher', 'Publication_Date', 'Pages', 'Dimensions'],\n",
      "      dtype='object')\n",
      "\n",
      " The new columns are: \n",
      " Index(['ID', 'Title', 'Author', 'Price', 'ISBN13', 'Publisher',\n",
      "       'Publication_Date', 'Pages', 'Dimensions'],\n",
      "      dtype='object') \n",
      " Index(['ID', 'Title', 'Author', 'Price', 'ISBN13', 'Publisher',\n",
      "       'Publication_Date', 'Pages', 'Dimensions'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# clean up\n",
    "print('The old columns are: \\n', tableA.columns, '\\n', tableB.columns)\n",
    "\n",
    "new_tableA = tableA\n",
    "new_tableB = tableB.drop(columns=['UsedPrice', 'ISBN10']).rename(columns={'NewPrice': 'Price'})\n",
    "\n",
    "new_tableA = new_tableA[['ID', 'Title', 'Author', 'Price', 'ISBN13', 'Publisher', 'Publication_Date', 'Pages', 'Dimensions']]\n",
    "new_tableB = new_tableB[['ID', 'Title', 'Author', 'Price', 'ISBN13', 'Publisher', 'Publication_Date', 'Pages', 'Dimensions']]\n",
    "\n",
    "print('\\n The new columns are: \\n', new_tableA.columns, '\\n', new_tableB.columns)\n",
    "\n",
    "# store new_tableA and new_tableB in the cleaned directory\n",
    "save_dir = 'cleaned/book/Barnes_and_Noble-Half'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "new_tableA.to_csv(os.path.join(save_dir, 'tableA.csv'), index=False)\n",
    "new_tableB.to_csv(os.path.join(save_dir, 'tableB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9bdedd37",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp_tableA = new_tableA[['ID', 'Title', 'ISBN13']].copy()\n",
    "temp_tableB = new_tableB[['ID', 'Title', 'ISBN13']].copy()\n",
    "\n",
    "connection = duckdb.connect(database=':memory:', read_only=False)\n",
    "connection.register('A', temp_tableA)\n",
    "connection.register('B', temp_tableB)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    ta.ID AS ID_A,\n",
    "    tb.ID AS ID_B,\n",
    "    ta.Title AS Title_A,\n",
    "    tb.Title AS Title_B,\n",
    "    ta.ISBN13 As ISBN13_A,\n",
    "    tb.ISBN13 As ISBN13_B\n",
    "    \n",
    "FROM\n",
    "    A ta\n",
    "CROSS JOIN\n",
    "    B tb\n",
    "WHERE (ta.ISBN13 == tb.ISBN13) OR\n",
    "    ((ta.ISBN13 IS NULL OR tb.ISBN13 IS NULL) AND (ta.Title == tb.Title))\n",
    "\"\"\"\n",
    "\n",
    "result = connection.execute(query).fetchdf()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "560f4b2d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## generate pos_candidates & neg_candidates, and store them in the cleaned directory\n",
    "pos_candidates = pd.DataFrame({\n",
    "    'ID_A': result['ID_A'], 'ID_B': result['ID_B'],\n",
    "    'Title_A': result['Title_A'], 'Title_B': result['Title_B'],\n",
    "    'ISBN13_A': result['ISBN13_A'], 'ISBN13_B': result['ISBN13_B'],\n",
    "})\n",
    "\n",
    "temp_tableA['temp_key'] = 1\n",
    "temp_tableB['temp_key'] = 1\n",
    "all_id_pairs = pd.merge(temp_tableA[['ID', 'temp_key']], temp_tableB[['ID', 'temp_key']], on='temp_key', suffixes=('_A', '_B'))\n",
    "all_id_pairs = all_id_pairs.drop('temp_key', axis=1)\n",
    "\n",
    "gold_match = gold_match.rename(columns={'ltable.ID': 'ID_A', 'rtable.ID': 'ID_B'})\n",
    "exclude_pairs = pd.concat([gold_match[['ID_A', 'ID_B']], pos_candidates[['ID_A', 'ID_B']]], axis=0).drop_duplicates()\n",
    "neg_candidates = pd.concat([all_id_pairs, exclude_pairs], axis=0).drop_duplicates(keep=False)\n",
    "\n",
    "neg_candidates = neg_candidates.sample(n=1000, random_state=SEED)\n",
    "neg_candidates = neg_candidates.merge(temp_tableA[['ID', 'Title', 'ISBN13']], \n",
    "                                      left_on='ID_A', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates.merge(temp_tableB[['ID', 'Title', 'ISBN13']], \n",
    "                                      left_on='ID_B', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates[list(pos_candidates.columns)]\n",
    "\n",
    "pos_candidates.to_csv(os.path.join(save_dir, 'pos_candidates.csv'), index=False)\n",
    "neg_candidates.to_csv(os.path.join(save_dir, 'neg_candidates.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb1e61",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "185bc3a2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Books 5\n",
    "\n",
    "The dataset consist of tableA from Amazon and tableB from Barnes and Noble.\n",
    "\n",
    "Metadata of the source data: The Amazon dataset comprises 2999 samples, while the Barnes and Noble dataset contains 2998 samples. Human annotators have labeled 40 pairs as gold-standard matches. Additionally, there are 205 pairs that exhibit an exact match at the string level in the 'title' column.\n",
    "\n",
    "\n",
    "The following are details to generate postive candidates (matches) and negative candidates (non-matches):\n",
    "\n",
    "**postive candidates** : \n",
    "1. The isbnA and isbnB are the same (when existing); OR\n",
    "2. The titleA contains titleB, vice verse.\n",
    "\n",
    "**negative candidates** :\n",
    "1. The idA and idB is distinct from the positive candidates; AND\n",
    "2. The idA and idB is distinct from the golden candidates.\n",
    "3. 1000 pairs are randonly selected afterward.\n",
    "\n",
    "The ['Title', 'ISBN'] columns are used for the later manual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "290d3230",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'books5/csv_files'\n",
    "tableA_path = os.path.join(src_book_dir_path, dataset_path, 'amazon.csv')\n",
    "tableB_path = os.path.join(src_book_dir_path, dataset_path, 'barnes_and_noble.csv')\n",
    "\n",
    "tableA = pd.read_csv(tableA_path)\n",
    "tableB = pd.read_csv(tableB_path)\n",
    "\n",
    "tableA.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "tableB.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "match_path = os.path.join(src_book_dir_path, dataset_path, 'labeled_data.csv')\n",
    "gold_match = pd.read_csv(match_path, skiprows=5)\n",
    "\n",
    "print(len(set(tableA.Title).intersection(set(tableB.title))))\n",
    "print(gold_match.Gold.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24908dda",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old columns are: \n",
      " Index(['ID', 'Title', 'Author', 'ISBN', 'Publisher', 'PublicationDate',\n",
      "       'Pages', 'price', 'ProductType'],\n",
      "      dtype='object') \n",
      " Index(['ID', 'title', 'authors', 'cover', 'pages', 'publisher', 'language',\n",
      "       'ISBN-10', 'ISBN13', 'price'],\n",
      "      dtype='object')\n",
      "\n",
      " The new columns are: \n",
      " Index(['ID', 'Title', 'Authors', 'ISBN', 'Publisher', 'Pages', 'Price'], dtype='object') \n",
      " Index(['ID', 'Title', 'Authors', 'ISBN', 'Publisher', 'Pages', 'Price'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# clean up\n",
    "print('The old columns are: \\n', tableA.columns, '\\n', tableB.columns)\n",
    "\n",
    "tableB['authors'] = tableB['authors'].apply(lambda x: x[:-1].replace(',', ', ') if not pd.isnull(x) else x)\n",
    "\n",
    "new_tableA = tableA.drop(columns=['ProductType', 'PublicationDate'])\\\n",
    "                   .rename(columns={'Author': 'Authors', 'price': 'Price'})\n",
    "new_tableB = tableB.drop(columns=['cover', 'language', 'ISBN-10'])\\\n",
    "                   .rename(columns={'title': 'Title', 'authors': 'Authors',\n",
    "                              'pages': 'Pages', 'publisher': 'Publisher',\n",
    "                              'price': 'Price', 'ISBN13': 'ISBN'})\n",
    "\n",
    "new_tableA = new_tableA[['ID', 'Title', 'Authors', 'ISBN', 'Publisher', 'Pages', 'Price']]\n",
    "new_tableB = new_tableB[['ID', 'Title', 'Authors', 'ISBN', 'Publisher', 'Pages', 'Price']]\n",
    "\n",
    "print('\\n The new columns are: \\n', new_tableA.columns, '\\n', new_tableB.columns)\n",
    "\n",
    "# store new_tableA and new_tableB in the cleaned directory\n",
    "save_dir = 'cleaned/book/Amazon-Barnes_and_Noble'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "new_tableA.to_csv(os.path.join(save_dir, 'tableA.csv'), index=False)\n",
    "new_tableB.to_csv(os.path.join(save_dir, 'tableB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac5b4a87",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp_tableA = new_tableA[['ID', 'Title', 'ISBN']].copy()\n",
    "temp_tableB = new_tableB[['ID', 'Title', 'ISBN']].copy()\n",
    "\n",
    "connection = duckdb.connect(database=':memory:', read_only=False)\n",
    "connection.register('A', temp_tableA)\n",
    "connection.register('B', temp_tableB)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    ta.ID AS ID_A,\n",
    "    tb.ID AS ID_B,\n",
    "    ta.Title AS Title_A,\n",
    "    tb.Title AS Title_B,\n",
    "    ta.ISBN As ISBN_A,\n",
    "    tb.ISBN As ISBN_B\n",
    "    \n",
    "FROM\n",
    "    A ta\n",
    "CROSS JOIN\n",
    "    B tb\n",
    "WHERE (ta.ISBN = tb.ISBN) OR\n",
    "    ((ta.ISBN IS NULL OR tb.ISBN IS NULL) AND ((LOWER(ta.Title) LIKE '%' || LOWER(tb.Title) || '%' OR LOWER(tb.Title) LIKE '%' || LOWER(ta.Title) || '%')))\n",
    "\"\"\"\n",
    "\n",
    "result = connection.execute(query).fetchdf()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31278c36",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_A</th>\n",
       "      <th>ID_B</th>\n",
       "      <th>Title_A</th>\n",
       "      <th>Title_B</th>\n",
       "      <th>ISBN_A</th>\n",
       "      <th>ISBN_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bn_804</td>\n",
       "      <td>B6</td>\n",
       "      <td>MySQL Crash Course</td>\n",
       "      <td>MySQL Crash Course</td>\n",
       "      <td>9780672327124</td>\n",
       "      <td>9.780672e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bn_1038</td>\n",
       "      <td>B10</td>\n",
       "      <td>Oracle8 Developer's Guide</td>\n",
       "      <td>Oracle8 Developer's Guide (Developer's Guides ...</td>\n",
       "      <td>9780764531972</td>\n",
       "      <td>9.780765e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bn_617</td>\n",
       "      <td>B15</td>\n",
       "      <td>High Availability Mysql Cookbook</td>\n",
       "      <td>High Availability MySQL Cookbook</td>\n",
       "      <td>9781847199942</td>\n",
       "      <td>9.781847e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bn_1609</td>\n",
       "      <td>B22</td>\n",
       "      <td>Automating Microsoft Access with Macros: For W...</td>\n",
       "      <td>Automating Microsoft Access With Macros: For W...</td>\n",
       "      <td>9780782118568</td>\n",
       "      <td>9.780782e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bn_276</td>\n",
       "      <td>B45</td>\n",
       "      <td>Oracle PL/SQL Programming</td>\n",
       "      <td>Oracle PL/SQL Programming: Guide to Oracle8i F...</td>\n",
       "      <td>9780596553142</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>bn_2570</td>\n",
       "      <td>B2881</td>\n",
       "      <td>Communicating Data with Tableau</td>\n",
       "      <td>Communicating Data with Tableau</td>\n",
       "      <td>9781449372026</td>\n",
       "      <td>9.781449e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>bn_2507</td>\n",
       "      <td>B2883</td>\n",
       "      <td>Creating a Self-Tuning Oracle Database: Automa...</td>\n",
       "      <td>Creating a Self-Tuning Oracle Database: Automa...</td>\n",
       "      <td>9780972751322</td>\n",
       "      <td>9.780973e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>bn_2757</td>\n",
       "      <td>B2885</td>\n",
       "      <td>Expert Oracle RAC 12c</td>\n",
       "      <td>Expert Oracle RAC 12c (The Expert's Voice)</td>\n",
       "      <td>9781430250449</td>\n",
       "      <td>9.781430e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>bn_2101</td>\n",
       "      <td>B2918</td>\n",
       "      <td>Expert One-on-One Microsoft Access Application...</td>\n",
       "      <td>Expert One-on-One Microsoft Access Application...</td>\n",
       "      <td>9780764559044</td>\n",
       "      <td>9.780765e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>bn_2350</td>\n",
       "      <td>B2966</td>\n",
       "      <td>Common Warehouse Metamodel</td>\n",
       "      <td>Common Warehouse Metamodel (OMG)</td>\n",
       "      <td>9780471200529</td>\n",
       "      <td>9.780471e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>566 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID_A   ID_B                                            Title_A  \\\n",
       "0     bn_804     B6                                 MySQL Crash Course   \n",
       "1    bn_1038    B10                          Oracle8 Developer's Guide   \n",
       "2     bn_617    B15                   High Availability Mysql Cookbook   \n",
       "3    bn_1609    B22  Automating Microsoft Access with Macros: For W...   \n",
       "4     bn_276    B45                          Oracle PL/SQL Programming   \n",
       "..       ...    ...                                                ...   \n",
       "561  bn_2570  B2881                    Communicating Data with Tableau   \n",
       "562  bn_2507  B2883  Creating a Self-Tuning Oracle Database: Automa...   \n",
       "563  bn_2757  B2885                              Expert Oracle RAC 12c   \n",
       "564  bn_2101  B2918  Expert One-on-One Microsoft Access Application...   \n",
       "565  bn_2350  B2966                         Common Warehouse Metamodel   \n",
       "\n",
       "                                               Title_B         ISBN_A  \\\n",
       "0                                   MySQL Crash Course  9780672327124   \n",
       "1    Oracle8 Developer's Guide (Developer's Guides ...  9780764531972   \n",
       "2                     High Availability MySQL Cookbook  9781847199942   \n",
       "3    Automating Microsoft Access With Macros: For W...  9780782118568   \n",
       "4    Oracle PL/SQL Programming: Guide to Oracle8i F...  9780596553142   \n",
       "..                                                 ...            ...   \n",
       "561                    Communicating Data with Tableau  9781449372026   \n",
       "562  Creating a Self-Tuning Oracle Database: Automa...  9780972751322   \n",
       "563         Expert Oracle RAC 12c (The Expert's Voice)  9781430250449   \n",
       "564  Expert One-on-One Microsoft Access Application...  9780764559044   \n",
       "565                   Common Warehouse Metamodel (OMG)  9780471200529   \n",
       "\n",
       "           ISBN_B  \n",
       "0    9.780672e+12  \n",
       "1    9.780765e+12  \n",
       "2    9.781847e+12  \n",
       "3    9.780782e+12  \n",
       "4             NaN  \n",
       "..            ...  \n",
       "561  9.781449e+12  \n",
       "562  9.780973e+12  \n",
       "563  9.781430e+12  \n",
       "564  9.780765e+12  \n",
       "565  9.780471e+12  \n",
       "\n",
       "[566 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39ce66c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## generate pos_candidates & neg_candidates, and store them in the cleaned directory\n",
    "pos_candidates = pd.DataFrame({\n",
    "    'ID_A': result['ID_A'], 'ID_B': result['ID_B'],\n",
    "    'Title_A': result['Title_A'], 'Title_B': result['Title_B'],\n",
    "    'ISBN_A': result['ISBN_A'], 'ISBN_B': result['ISBN_B'],\n",
    "})\n",
    "\n",
    "temp_tableA['temp_key'] = 1\n",
    "temp_tableB['temp_key'] = 1\n",
    "all_id_pairs = pd.merge(temp_tableA[['ID', 'temp_key']], temp_tableB[['ID', 'temp_key']], on='temp_key', suffixes=('_A', '_B'))\n",
    "all_id_pairs = all_id_pairs.drop('temp_key', axis=1)\n",
    "\n",
    "gold_match = gold_match.rename(columns={'ltable.ID': 'ID_A', 'rtable.ID': 'ID_B'})\n",
    "exclude_pairs = pd.concat([gold_match[['ID_A', 'ID_B']], pos_candidates[['ID_A', 'ID_B']]], axis=0).drop_duplicates()\n",
    "neg_candidates = pd.concat([all_id_pairs, exclude_pairs], axis=0).drop_duplicates(keep=False)\n",
    "\n",
    "neg_candidates = neg_candidates.sample(n=1000, random_state=SEED)\n",
    "neg_candidates = neg_candidates.merge(temp_tableA[['ID', 'Title', 'ISBN']], \n",
    "                                      left_on='ID_A', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates.merge(temp_tableB[['ID', 'Title', 'ISBN']], \n",
    "                                      left_on='ID_B', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates[list(pos_candidates.columns)]\n",
    "\n",
    "pos_candidates.to_csv(os.path.join(save_dir, 'pos_candidates.csv'), index=False)\n",
    "neg_candidates.to_csv(os.path.join(save_dir, 'neg_candidates.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab94e6c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02d18d04",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Paper Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83657e69",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "src_paper_dir_path = 'dirty/paper'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bb7f5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### AcadPapers\n",
    "\n",
    "The dataset consist of tableA from arxiv and tableB from neurips.\n",
    "\n",
    "Metadata of the source data: The arxiv dataset comprises 4000 samples, while the neurips dataset contains 4397 samples. There are 96 pairs that exhibit an exact match at the string level in the 'title' column, and no golden labeled are given.\n",
    "\n",
    "\n",
    "The following are details to generate postive candidates (matches) and negative candidates (non-matches):\n",
    "\n",
    "**postive candidates** : \n",
    "1. The titleA and titleB are the same; OR\n",
    "2. The titleA and titleB are significant similar (similarity larger than 0.7, determined by their overlapping size).\n",
    "3. Roughly 1100 pairs are randonly selected afterward.\n",
    "\n",
    "**negative candidates** :\n",
    "1. The idA and idB is distinct from the positive candidates; AND\n",
    "2. 1000 pairs are randonly selected afterward.\n",
    "\n",
    "The ['Title', 'Authors'] columns are used for the later manual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c69dbd7a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = 'AcadPapers'\n",
    "tableA_path = os.path.join(src_paper_dir_path, dataset_path, 'table_a.csv')\n",
    "tableB_path = os.path.join(src_paper_dir_path, dataset_path, 'table_b.csv')\n",
    "\n",
    "tableA = pd.read_csv(tableA_path)\n",
    "tableB = pd.read_csv(tableB_path)\n",
    "\n",
    "tableA.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "tableB.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "tableA\n",
    "len(set(tableA.Title).intersection(set(tableB.Title)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cec3771f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old columns are: \n",
      " Index(['_id', 'Title', 'Authors', 'Date', 'Abstract', 'Journal-ref'], dtype='object') \n",
      " Index(['_id', 'Title', 'Authors', 'Date', 'Abstract', 'Journal-ref'], dtype='object')\n",
      "\n",
      " The new columns are: \n",
      " Index(['ID', 'Title', 'Authors', 'Date', 'Journal-ref'], dtype='object') \n",
      " Index(['ID', 'Title', 'Authors', 'Date', 'Journal-ref'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# clean up\n",
    "print('The old columns are: \\n', tableA.columns, '\\n', tableB.columns)\n",
    "\n",
    "new_tableA = tableA.drop(columns=['Abstract']).rename(columns={'_id': 'ID'})\n",
    "new_tableB = tableB.drop(columns=['Abstract']).rename(columns={'_id': 'ID'})\n",
    "\n",
    "print('\\n The new columns are: \\n', new_tableA.columns, '\\n', new_tableB.columns)\n",
    "\n",
    "# store new_tableA and new_tableB in the cleaned directory\n",
    "save_dir = 'cleaned/paper/Arxiv-Neurips'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "new_tableA.to_csv(os.path.join(save_dir, 'tableA.csv'), index=False)\n",
    "new_tableB.to_csv(os.path.join(save_dir, 'tableB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "178988dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp_tableA = new_tableA[['ID', 'Title', 'Authors']].copy()\n",
    "temp_tableB = new_tableB[['ID', 'Title', 'Authors']].copy()\n",
    "\n",
    "temp_tableA['key'] = 1\n",
    "temp_tableB['key'] = 1\n",
    "\n",
    "cross_joined_df = pd.merge(temp_tableA, temp_tableB, on='key', suffixes=('_A', '_B')).drop('key', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04b711d9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_similarity(title_a, title_b):\n",
    "    set_a = set(title_a.lower().split())\n",
    "    set_b = set(title_b.lower().split())\n",
    "    intersection = set_a.intersection(set_b)\n",
    "    smaller_set_size = min(len(set_a), len(set_b))\n",
    "    if smaller_set_size > 0:\n",
    "        return len(intersection) / smaller_set_size\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "cross_joined_df['similarity'] = cross_joined_df.apply(lambda row: compute_similarity(row['Title_A'], row['Title_B']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6131b1a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "high_sim_df = cross_joined_df[cross_joined_df['similarity']>0.7]\n",
    "pos_candidates = pd.concat([high_sim_df[high_sim_df['similarity']==1],\n",
    "                            high_sim_df[high_sim_df['similarity']!=1].sample(n=900, random_state=SEED)], axis=0)\n",
    "pos_candidates = pos_candidates[['ID_A', 'ID_B', 'Title_A', 'Title_B', 'Authors_A', 'Authors_B']]\n",
    "\n",
    "low_sim_df = cross_joined_df[cross_joined_df['similarity']<=0.7]\n",
    "neg_candidates = low_sim_df.sample(n=1000, random_state=SEED)\n",
    "neg_candidates = neg_candidates[['ID_A', 'ID_B', 'Title_A', 'Title_B', 'Authors_A', 'Authors_B']]\n",
    "\n",
    "pos_candidates.to_csv(os.path.join(save_dir, 'pos_candidates.csv'), index=False)\n",
    "neg_candidates.to_csv(os.path.join(save_dir, 'neg_candidates.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf295ed6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93a25061",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Citations\n",
    "\n",
    "The dataset consist of tableA from Google Scholar and tableB from DBLP.\n",
    "\n",
    "Metadata of the source data: The Google Scholar dataset comprises 64263 samples, while the DBLP dataset contains 2616 samples. Human annotators have labeled 5347 pairs as gold-standard matches. Additionally, there are 1709 pairs that exhibit an exact match at the string level in the 'title' column.\n",
    "\n",
    "\n",
    "The following are details to generate postive candidates (matches) and negative candidates (non-matches):\n",
    "\n",
    "**postive candidates** : \n",
    "1. The titleA and titleB are the same; AND\n",
    "2. The yearA and yearB are the same (when existing).\n",
    "3. 1100 pairs are randonly selected afterward.\n",
    "\n",
    "**negative candidates** :\n",
    "1. The idA and idB is distinct from the positive candidates; AND\n",
    "2. The idA and idB is distinct from the golden candidates.\n",
    "3. 1000 pairs are randonly selected afterward.\n",
    "\n",
    "The ['Title', 'Authors'] columns are used for the later manual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "973a619b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1709\n",
      "(5347, 2)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'citations'\n",
    "tableA_path = os.path.join(src_paper_dir_path, dataset_path, 'google_scholar.csv')\n",
    "tableB_path = os.path.join(src_paper_dir_path, dataset_path, 'dblp.csv')\n",
    "\n",
    "tableA = pd.read_csv(tableA_path)\n",
    "tableB = pd.read_csv(tableB_path)\n",
    "\n",
    "tableA.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "tableB.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "match_path = os.path.join(src_paper_dir_path, dataset_path, 'matches_dblp_scholar.csv')\n",
    "gold_match = pd.read_csv(match_path)\n",
    "\n",
    "print(len(set(tableA.title).intersection(set(tableB.title))))\n",
    "print(gold_match.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "102523e3",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old columns are: \n",
      " Index(['id', 'title', 'authors', 'venue', 'year'], dtype='object') \n",
      " Index(['id', 'title', 'authors', 'venue', 'year'], dtype='object')\n",
      "\n",
      " The new columns are: \n",
      " Index(['ID', 'Title', 'Authors', 'Venue', 'Year'], dtype='object') \n",
      " Index(['ID', 'Title', 'Authors', 'Venue', 'Year'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# clean up\n",
    "print('The old columns are: \\n', tableA.columns, '\\n', tableB.columns)\n",
    "\n",
    "new_tableA = tableA.rename(columns={'id': 'ID', 'title': 'Title', 'authors': 'Authors', \n",
    "                                    'venue': 'Venue', 'year': 'Year'})\n",
    "\n",
    "new_tableB = tableB.rename(columns={'id': 'ID', 'title': 'Title', 'authors': 'Authors', \n",
    "                                    'venue': 'Venue', 'year': 'Year'})\n",
    "\n",
    "print('\\n The new columns are: \\n', new_tableA.columns, '\\n', new_tableB.columns)\n",
    "\n",
    "# store new_tableA and new_tableB in the cleaned directory\n",
    "save_dir = 'cleaned/paper/Google_Scholar-DBLP'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "new_tableA.to_csv(os.path.join(save_dir, 'tableA.csv'), index=False)\n",
    "new_tableB.to_csv(os.path.join(save_dir, 'tableB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a09c3df",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp_tableA = new_tableA[['ID', 'Title', 'Year', 'Authors']]\n",
    "temp_tableB = new_tableB[['ID', 'Title', 'Year', 'Authors']]\n",
    "\n",
    "connection = duckdb.connect(database=':memory:', read_only=False)\n",
    "connection.register('A', temp_tableA)\n",
    "connection.register('B', temp_tableB)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    ta.ID AS ID_A,\n",
    "    tb.ID AS ID_B,\n",
    "    ta.Title AS Title_A,\n",
    "    tb.Title AS Title_B,\n",
    "    ta.Year AS Year_A,\n",
    "    tb.Year AS Year_B,\n",
    "    ta.Authors AS Authors_A,\n",
    "    tb.Authors AS Authors_B\n",
    "    \n",
    "FROM\n",
    "    A ta\n",
    "CROSS JOIN\n",
    "    B tb\n",
    "WHERE\n",
    "    (ta.Title == tb.Title) AND ((ta.Year IS NULL OR tb.Year IS NULL) OR ta.Year == tb.Year)\n",
    "\"\"\"\n",
    "\n",
    "result = connection.execute(query).fetchdf()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb1e0ab5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## generate pos_candidates & neg_candidates, and store them in the cleaned directory\n",
    "pos_candidates = pd.DataFrame({\n",
    "    'ID_A': result['ID_A'], 'ID_B': result['ID_B'],\n",
    "    'Title_A': result['Title_A'], 'Title_B': result['Title_B'],\n",
    "    'Year_A': result['Year_A'], 'Year_B': result['Year_B'],\n",
    "    'Authors_A': result['Authors_A'], 'Authors_B': result['Authors_B'],\n",
    "})\n",
    "\n",
    "temp_tableA['temp_key'] = 1\n",
    "temp_tableB['temp_key'] = 1\n",
    "all_id_pairs = pd.merge(temp_tableA[['ID', 'temp_key']], temp_tableB[['ID', 'temp_key']], on='temp_key', suffixes=('_A', '_B'))\n",
    "all_id_pairs = all_id_pairs.drop('temp_key', axis=1)\n",
    "\n",
    "gold_match = gold_match.rename(columns={'google_scholar_id': 'ID_A', 'dblp_id': 'ID_B'})\n",
    "exclude_pairs = pd.concat([gold_match[['ID_A', 'ID_B']], pos_candidates[['ID_A', 'ID_B']]], axis=0).drop_duplicates()\n",
    "neg_candidates = pd.concat([all_id_pairs, exclude_pairs], axis=0).drop_duplicates(keep=False)\n",
    "\n",
    "pos_candidates = pos_candidates.sample(n=1100, random_state=SEED)\n",
    "neg_candidates = neg_candidates.sample(n=1000, random_state=SEED)\n",
    "\n",
    "neg_candidates = neg_candidates.merge(temp_tableA[['ID', 'Title', 'Year', 'Authors']], \n",
    "                                      left_on='ID_A', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates.merge(temp_tableB[['ID', 'Title', 'Year', 'Authors']], \n",
    "                                      left_on='ID_B', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates[list(pos_candidates.columns)]\n",
    "\n",
    "pos_candidates.to_csv(os.path.join(save_dir, 'pos_candidates.csv'), index=False)\n",
    "neg_candidates.to_csv(os.path.join(save_dir, 'neg_candidates.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dba76ad",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f168701",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### CompVision\n",
    "\n",
    "The dataset consist of tableA from arxiv and tableB from thecvf.\n",
    "\n",
    "Metadata of the source data: The arxiv dataset comprises 3547 samples, while the thecvf dataset contains 10000 samples. There are 340 pairs that exhibit an exact match at the string level in the 'title' column, and no golden labeled are given.\n",
    "\n",
    "\n",
    "The following are details to generate postive candidates (matches) and negative candidates (non-matches):\n",
    "\n",
    "**postive candidates** : \n",
    "1. The titleA and titleB are the same; OR\n",
    "2. The titleA and titleB are significant similar (similarity larger than 0.7, determined by their overlapping size).\n",
    "3. Roughly 1100 pairs are randonly selected afterward.\n",
    "\n",
    "**negative candidates** :\n",
    "1. The idA and idB is distinct from the positive candidates; AND\n",
    "2. 1000 pairs are randonly selected afterward.\n",
    "\n",
    "The ['Title', 'Authors'] columns are used for the later manual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1a7c104",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'CompVision'\n",
    "tableA_path = os.path.join(src_paper_dir_path, dataset_path, 'table_a.csv')\n",
    "tableB_path = os.path.join(src_paper_dir_path, dataset_path, 'table_b.csv')\n",
    "\n",
    "tableA = pd.read_csv(tableA_path)\n",
    "tableB = pd.read_csv(tableB_path)\n",
    "\n",
    "tableA.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "tableB.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "print(len(set(tableA.Title).intersection(set(tableB.Title))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "177d8b6d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old columns are: \n",
      " Index(['_id', 'Tag', 'Title', 'Authors', 'Month', 'Year', 'JournalRef'], dtype='object') \n",
      " Index(['_id', 'Tag', 'Title', 'Authors', 'Month', 'Year', 'JournalRef'], dtype='object')\n",
      "\n",
      " The new columns are: \n",
      " Index(['ID', 'Tag', 'Title', 'Authors', 'Month', 'Year', 'JournalRef'], dtype='object') \n",
      " Index(['ID', 'Tag', 'Title', 'Authors', 'Month', 'Year', 'JournalRef'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# clean up\n",
    "print('The old columns are: \\n', tableA.columns, '\\n', tableB.columns)\n",
    "\n",
    "new_tableA = tableA.rename(columns={'_id': 'ID'})\n",
    "\n",
    "new_tableB = tableB.rename(columns={'_id': 'ID'})\n",
    "\n",
    "print('\\n The new columns are: \\n', new_tableA.columns, '\\n', new_tableB.columns)\n",
    "\n",
    "# store new_tableA and new_tableB in the cleaned directory\n",
    "save_dir = 'cleaned/paper/Arxiv-THECVF'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "new_tableA.to_csv(os.path.join(save_dir, 'tableA.csv'), index=False)\n",
    "new_tableB.to_csv(os.path.join(save_dir, 'tableB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b5f675e3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp_tableA = new_tableA[['ID', 'Title', 'Authors']].copy()\n",
    "temp_tableB = new_tableB[['ID', 'Title', 'Authors']].copy()\n",
    "\n",
    "temp_tableA['key'] = 1\n",
    "temp_tableB['key'] = 1\n",
    "\n",
    "cross_joined_df = pd.merge(temp_tableA, temp_tableB, on='key', suffixes=('_A', '_B')).drop('key', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c2277689",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_similarity(title_a, title_b):\n",
    "    set_a = set(title_a.lower().split())\n",
    "    set_b = set(title_b.lower().split())\n",
    "    intersection = set_a.intersection(set_b)\n",
    "    smaller_set_size = min(len(set_a), len(set_b))\n",
    "    if smaller_set_size > 0:\n",
    "        return len(intersection) / smaller_set_size\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "cross_joined_df['similarity'] = cross_joined_df.apply(lambda row: compute_similarity(row['Title_A'], row['Title_B']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97e42a1e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "high_sim_df = cross_joined_df[cross_joined_df['similarity']>0.7]\n",
    "pos_candidates = pd.concat([high_sim_df[high_sim_df['similarity']==1],\n",
    "                            high_sim_df[high_sim_df['similarity']!=1].sample(n=600, random_state=SEED)], axis=0)\n",
    "pos_candidates = pos_candidates[['ID_A', 'ID_B', 'Title_A', 'Title_B', 'Authors_A', 'Authors_B']]\n",
    "\n",
    "low_sim_df = cross_joined_df[cross_joined_df['similarity']<=0.7]\n",
    "neg_candidates = low_sim_df.sample(n=1000, random_state=SEED)\n",
    "neg_candidates = neg_candidates[['ID_A', 'ID_B', 'Title_A', 'Title_B', 'Authors_A', 'Authors_B']]\n",
    "\n",
    "pos_candidates.to_csv(os.path.join(save_dir, 'pos_candidates.csv'), index=False)\n",
    "neg_candidates.to_csv(os.path.join(save_dir, 'neg_candidates.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6245702",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97bbca7a",
   "metadata": {},
   "source": [
    "## Movie Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b0d117bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_movie_dir_path = 'dirty/movie'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23652209",
   "metadata": {},
   "source": [
    "### Movies 1\n",
    "\n",
    "The dataset consist of tableA from Rotten Tomatoes and tableB from IMDB.\n",
    "\n",
    "Metadata of the source data: The Rotten Tomatoes dataset comprises 7390 samples, while the IMDB dataset contains 6407 samples. Human annotators have labeled 190 pairs as gold-standard matches. Additionally, there are 3466 pairs that exhibit an exact match at the string level in the 'name' column.\n",
    "\n",
    "\n",
    "The following are details to generate postive candidates (matches) and negative candidates (non-matches):\n",
    "\n",
    "**postive candidates** : \n",
    "1. The nameA and nameB are the same; AND\n",
    "2. The directorA and directorB are the same.\n",
    "3. 1100 pairs are randonly selected afterward.\n",
    "\n",
    "**negative candidates** :\n",
    "1. The idA and idB is distinct from the positive candidates; AND\n",
    "2. The idA and idB is distinct from the golden candidates.\n",
    "3. 1000 pairs are randonly selected afterward.\n",
    "\n",
    "The ['Name', 'Director', 'Year'] columns are used for the later manual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4fb2c046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3466\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'movies1/csv_files'\n",
    "tableA_path = os.path.join(src_movie_dir_path, dataset_path, 'rotten_tomatoes.csv')\n",
    "tableB_path = os.path.join(src_movie_dir_path, dataset_path, 'imdb.csv')\n",
    "\n",
    "tableA = pd.read_csv(tableA_path)\n",
    "tableB = pd.read_csv(tableB_path)\n",
    "\n",
    "tableA.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "tableB.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "match_path = os.path.join(src_movie_dir_path, dataset_path, 'labeled_data.csv')\n",
    "gold_match = pd.read_csv(match_path, skiprows=5)\n",
    "\n",
    "print(len(set(tableA.Name).intersection(set(tableB.Name))))\n",
    "print(gold_match.gold.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "20d16fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old columns are: \n",
      " Index(['Id', 'Name', 'Year', 'Release Date', 'Director', 'Creator', 'Actors',\n",
      "       'Cast', 'Language', 'Country', 'Duration', 'RatingValue', 'RatingCount',\n",
      "       'ReviewCount', 'Genre', 'Filming Locations', 'Description'],\n",
      "      dtype='object') \n",
      " Index(['Id', 'Name', 'YearRange', 'ReleaseDate', 'Director', 'Creator', 'Cast',\n",
      "       'Duration', 'RatingValue', 'ContentRating', 'Genre', 'Url',\n",
      "       'Description'],\n",
      "      dtype='object')\n",
      "\n",
      " The new columns are: \n",
      " Index(['ID', 'Name', 'Director', 'Creator', 'Duration', 'Rating', 'Year',\n",
      "       'Release_Date', 'Genre'],\n",
      "      dtype='object') \n",
      " Index(['ID', 'Name', 'Director', 'Creator', 'Duration', 'Rating', 'Year',\n",
      "       'Release_Date', 'Genre'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# clean up\n",
    "print('The old columns are: \\n', tableA.columns, '\\n', tableB.columns)\n",
    "\n",
    "new_tableA = tableA.drop(columns=['Actors', 'Cast', 'Language', 'Country', 'RatingCount', \n",
    "                                  'ReviewCount', 'Filming Locations', 'Description'])\\\n",
    "                   .rename(columns={'Id': 'ID', 'Release Date': 'Release_Date', 'RatingValue': 'Rating'})\n",
    "\n",
    "new_tableB = tableB.drop(columns=['Cast', 'Url', 'Description', 'ContentRating'])\\\n",
    "                   .rename(columns={'Id': 'ID', 'ReleaseDate': 'Release_Date', 'YearRange': 'Year',\n",
    "                                    'RatingValue': 'Rating'})\n",
    "new_tableA = new_tableA[['ID', 'Name', 'Director', 'Creator', 'Duration', 'Rating', 'Year', 'Release_Date', 'Genre']]\n",
    "new_tableB = new_tableB[['ID', 'Name', 'Director', 'Creator', 'Duration', 'Rating', 'Year', 'Release_Date', 'Genre']]\n",
    "\n",
    "print('\\n The new columns are: \\n', new_tableA.columns, '\\n', new_tableB.columns)\n",
    "\n",
    "# store new_tableA and new_tableB in the cleaned directory\n",
    "save_dir = 'cleaned/movie/Rotten_Tomatoes-IMDB'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "new_tableA.to_csv(os.path.join(save_dir, 'tableA.csv'), index=False)\n",
    "new_tableB.to_csv(os.path.join(save_dir, 'tableB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7004ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_tableA = new_tableA[['ID', 'Name', 'Director', 'Year']].copy()\n",
    "temp_tableB = new_tableB[['ID', 'Name', 'Director', 'Year']].copy()\n",
    "\n",
    "connection = duckdb.connect(database=':memory:', read_only=False)\n",
    "connection.register('A', temp_tableA)\n",
    "connection.register('B', temp_tableB)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    ta.ID AS ID_A,\n",
    "    tb.ID AS ID_B,\n",
    "    ta.Name AS Name_A,\n",
    "    tb.Name AS Name_B,\n",
    "    ta.Director As Director_A,\n",
    "    tb.Director As Director_B,\n",
    "    ta.Year As Year_A,\n",
    "    tb.Year As Year_B,\n",
    "    \n",
    "FROM\n",
    "    A ta\n",
    "CROSS JOIN\n",
    "    B tb\n",
    "WHERE\n",
    "    ta.Name == tb.Name AND ta.Director == tb.Director\n",
    "\"\"\"\n",
    "\n",
    "result = connection.execute(query).fetchdf()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b7fc12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate pos_candidates & neg_candidates, and store them in the cleaned directory\n",
    "pos_candidates = pd.DataFrame({\n",
    "    'ID_A': result['ID_A'], 'ID_B': result['ID_B'],\n",
    "    'Name_A': result['Name_A'], 'Name_B': result['Name_B'],\n",
    "    'Director_A': result['Director_A'], 'Director_B': result['Director_B'],\n",
    "    'Year_A': result['Year_A'], 'Year_B': result['Year_B'],\n",
    "})\n",
    "\n",
    "temp_tableA['temp_key'] = 1\n",
    "temp_tableB['temp_key'] = 1\n",
    "all_id_pairs = pd.merge(temp_tableA[['ID', 'temp_key']], temp_tableB[['ID', 'temp_key']], on='temp_key', suffixes=('_A', '_B'))\n",
    "all_id_pairs = all_id_pairs.drop('temp_key', axis=1)\n",
    "\n",
    "gold_match = gold_match.rename(columns={'ltable.Id': 'ID_A', 'rtable.Id': 'ID_B'})\n",
    "exclude_pairs = pd.concat([gold_match[['ID_A', 'ID_B']], pos_candidates[['ID_A', 'ID_B']]], axis=0).drop_duplicates()\n",
    "neg_candidates = pd.concat([all_id_pairs, exclude_pairs], axis=0).drop_duplicates(keep=False)\n",
    "\n",
    "pos_candidates = pos_candidates.sample(n=1100, random_state=SEED)\n",
    "neg_candidates = neg_candidates.sample(n=1000, random_state=SEED)\n",
    "\n",
    "neg_candidates = neg_candidates.merge(temp_tableA[['ID', 'Name', 'Director', 'Year']], \n",
    "                                      left_on='ID_A', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates.merge(temp_tableB[['ID', 'Name', 'Director', 'Year']], \n",
    "                                      left_on='ID_B', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates[list(pos_candidates.columns)]\n",
    "\n",
    "pos_candidates.to_csv(os.path.join(save_dir, 'pos_candidates.csv'), index=False)\n",
    "neg_candidates.to_csv(os.path.join(save_dir, 'neg_candidates.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a19e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3fa6994",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Movies 4\n",
    "\n",
    "The dataset consist of tableA from Amazon and tableB from Rotten Tomatoes.\n",
    "\n",
    "Metadata of the source data: The Amazon dataset comprises 5241 samples, while the Rotten Tomatoes dataset contains 4392 samples. Human annotators have labeled 53 pairs as gold-standard matches. Additionally, there are 27 pairs that exhibit an exact match at the string level in the 'title' column.\n",
    "\n",
    "\n",
    "The following are details to generate postive candidates (matches) and negative candidates (non-matches):\n",
    "\n",
    "**postive candidates** : \n",
    "1. The titleA contains titleB, vice verse; AND\n",
    "2. The directorA and directorB are the same.\n",
    "3. 1100 pairs are randonly selected afterward.\n",
    "\n",
    "**negative candidates** :\n",
    "1. The idA and idB is distinct from the positive candidates; AND\n",
    "2. The idA and idB is distinct from the golden candidates.\n",
    "3. 1000 pairs are randonly selected afterward.\n",
    "\n",
    "The ['Title', 'Director', 'Year'] columns are used for the later manual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "78019715",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'movies4/csv_files'\n",
    "tableA_path = os.path.join(src_movie_dir_path, dataset_path, 'amazon.csv')\n",
    "tableB_path = os.path.join(src_movie_dir_path, dataset_path, 'rotten_tomatoes.csv')\n",
    "\n",
    "tableA = pd.read_csv(tableA_path)\n",
    "tableB = pd.read_csv(tableB_path, encoding='latin1')\n",
    "\n",
    "tableA.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "tableB.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "match_path = os.path.join(src_movie_dir_path, dataset_path, 'labeled_data.csv')\n",
    "gold_match = pd.read_csv(match_path, skiprows=5)\n",
    "\n",
    "print(len(set(tableA.title).intersection(set(tableB.title))))\n",
    "print(gold_match.gold.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "78e15a7b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old columns are: \n",
      " Index(['id', 'title', 'time', 'director', 'year', 'star', 'cost'], dtype='object') \n",
      " Index(['id', 'title', 'time', 'director', 'year', 'star1', 'star2', 'star3',\n",
      "       'star4', 'star5', 'star6', 'rotten_tomatoes', 'audience_rating',\n",
      "       'review1', 'review2', 'review3', 'review4', 'review5', 'cast'],\n",
      "      dtype='object')\n",
      "\n",
      " The new columns are: \n",
      " Index(['ID', 'Title', 'Time', 'Director', 'Year', 'Cast'], dtype='object') \n",
      " Index(['ID', 'Title', 'Time', 'Director', 'Year', 'Cast'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# clean up\n",
    "print('The old columns are: \\n', tableA.columns, '\\n', tableB.columns)\n",
    "\n",
    "tableB['cast'] = tableB.apply(lambda x: ', '.join([x[col] for col in \n",
    "                                                 ['star1', 'star2', 'star3', 'star4', 'star5', 'star6'] \n",
    "                                                 if pd.notnull(x[col])]), axis=1)\n",
    "\n",
    "new_tableA = tableA.drop(columns=['cost'])\\\n",
    "                   .rename(columns={'id': 'ID', 'title': 'Title', 'time': 'Time', 'director': 'Director', \n",
    "                                    'year': 'Year', 'star': 'Cast'})\n",
    "\n",
    "new_tableB = tableB.drop(columns=['star1', 'star2', 'star3', 'star4', 'star5', 'star6', 'review1', 'review2',\n",
    "                                  'review3', 'review4', 'review5', 'rotten_tomatoes', 'audience_rating'])\\\n",
    "                   .rename(columns={'id': 'ID', 'title': 'Title', 'time': 'Time', 'director': 'Director', \n",
    "                                    'year': 'Year', 'cast': 'Cast'})\n",
    "\n",
    "print('\\n The new columns are: \\n', new_tableA.columns, '\\n', new_tableB.columns)\n",
    "\n",
    "# store new_tableA and new_tableB in the cleaned directory\n",
    "save_dir = 'cleaned/movie/Amazon-Rotten_Tomatoes'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "new_tableA.to_csv(os.path.join(save_dir, 'tableA.csv'), index=False)\n",
    "new_tableB.to_csv(os.path.join(save_dir, 'tableB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e64a1552",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp_tableA = new_tableA[['ID', 'Title', 'Director', 'Year']].copy()\n",
    "temp_tableB = new_tableB[['ID', 'Title', 'Director', 'Year']].copy()\n",
    "\n",
    "connection = duckdb.connect(database=':memory:', read_only=False)\n",
    "connection.register('A', temp_tableA)\n",
    "connection.register('B', temp_tableB)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    ta.ID AS ID_A,\n",
    "    tb.ID AS ID_B,\n",
    "    ta.Title AS Title_A,\n",
    "    tb.Title AS Title_B,\n",
    "    ta.Director As Director_A,\n",
    "    tb.Director As Director_B,\n",
    "    ta.Year As Year_A,\n",
    "    tb.Year As Year_B,\n",
    "    \n",
    "FROM\n",
    "    A ta\n",
    "CROSS JOIN\n",
    "    B tb\n",
    "WHERE \n",
    "    ((ta.Title LIKE '%' || tb.Title || '%') OR (tb.Title LIKE '%' || ta.Title || '%')) AND\n",
    "    (ta.Director == tb.Director)\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "result = connection.execute(query).fetchdf()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ad2097a5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## generate pos_candidates & neg_candidates, and store them in the cleaned directory\n",
    "pos_candidates = pd.DataFrame({\n",
    "    'ID_A': result['ID_A'], 'ID_B': result['ID_B'],\n",
    "    'Title_A': result['Title_A'], 'Title_B': result['Title_B'],\n",
    "    'Director_A': result['Director_A'], 'Director_B': result['Director_B'],\n",
    "    'Year_A': result['Year_A'], 'Year_B': result['Year_B'],\n",
    "})\n",
    "\n",
    "temp_tableA['temp_key'] = 1\n",
    "temp_tableB['temp_key'] = 1\n",
    "all_id_pairs = pd.merge(temp_tableA[['ID', 'temp_key']], temp_tableB[['ID', 'temp_key']], on='temp_key', suffixes=('_A', '_B'))\n",
    "all_id_pairs = all_id_pairs.drop('temp_key', axis=1)\n",
    "\n",
    "gold_match = gold_match.rename(columns={'ltable.id': 'ID_A', 'rtable.id': 'ID_B'})\n",
    "exclude_pairs = pd.concat([gold_match[['ID_A', 'ID_B']], pos_candidates[['ID_A', 'ID_B']]], axis=0).drop_duplicates()\n",
    "neg_candidates = pd.concat([all_id_pairs, exclude_pairs], axis=0).drop_duplicates(keep=False)\n",
    "\n",
    "pos_candidates = pos_candidates.sample(n=1100, random_state=SEED)\n",
    "neg_candidates = neg_candidates.sample(n=1000, random_state=SEED)\n",
    "\n",
    "neg_candidates = neg_candidates.merge(temp_tableA[['ID', 'Title', 'Director', 'Year']], \n",
    "                                      left_on='ID_A', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates.merge(temp_tableB[['ID', 'Title', 'Director', 'Year']], \n",
    "                                      left_on='ID_B', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates[list(pos_candidates.columns)]\n",
    "\n",
    "pos_candidates.to_csv(os.path.join(save_dir, 'pos_candidates.csv'), index=False)\n",
    "neg_candidates.to_csv(os.path.join(save_dir, 'neg_candidates.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e5edc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "143153bf",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Movies 5\n",
    "\n",
    "The dataset consist of tableA from Roger Ebert and tableB from IMDB.\n",
    "\n",
    "Metadata of the source data: The Roger Ebert dataset comprises 3556 samples, while the IMDB dataset contains 6913 samples. Human annotators have labeled 241 pairs as gold-standard matches. Additionally, there are 362 pairs that exhibit an exact match at the string level in the 'title' column.\n",
    "\n",
    "\n",
    "The following are details to generate postive candidates (matches) and negative candidates (non-matches):\n",
    "\n",
    "**postive candidates** : \n",
    "1. The titleA and titleB are significant similar (similarity larger than 0.7, determined by their overlapping size); AND\n",
    "2. The yearA and yearB are the same (when existing).\n",
    "\n",
    "**negative candidates** :\n",
    "1. The idA and idB is distinct from the positive candidates; AND\n",
    "3. 1000 pairs are randonly selected afterward.\n",
    "\n",
    "The ['Title', 'Directors', 'Year'] columns are used for the later manual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1b813c9a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n",
      "241\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'movies5/csv_files'\n",
    "tableA_path = os.path.join(src_movie_dir_path, dataset_path, 'roger_ebert.csv')\n",
    "tableB_path = os.path.join(src_movie_dir_path, dataset_path, 'imdb.csv')\n",
    "\n",
    "tableA = pd.read_csv(tableA_path)\n",
    "tableB = pd.read_csv(tableB_path)\n",
    "\n",
    "tableA.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "tableB.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "match_path = os.path.join(src_movie_dir_path, dataset_path, 'labeled_data.csv')\n",
    "gold_match = pd.read_csv(match_path, skiprows=5)\n",
    "\n",
    "print(len(set(tableA.movie_name).intersection(set(tableB.movie_name))))\n",
    "print(gold_match.gold.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "891c7647",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old columns are: \n",
      " Index(['id', 'movie_name', 'year', 'directors', 'actors', 'critic_rating',\n",
      "       'genre', 'pg_rating', 'duration'],\n",
      "      dtype='object') \n",
      " Index(['id', 'movie_name', 'year', 'directors', 'actors', 'movie_rating',\n",
      "       'genre', 'duration'],\n",
      "      dtype='object')\n",
      "\n",
      " The new columns are: \n",
      " Index(['ID', 'Title', 'Year', 'Directors', 'Actors', 'Content_Rating', 'Genre',\n",
      "       'Duration'],\n",
      "      dtype='object') \n",
      " Index(['ID', 'Title', 'Year', 'Directors', 'Actors', 'Content_Rating', 'Genre',\n",
      "       'Duration'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# clean up\n",
    "print('The old columns are: \\n', tableA.columns, '\\n', tableB.columns)\n",
    "\n",
    "new_tableA = tableA.drop(columns=['pg_rating'])\\\n",
    "                   .rename(columns={'id': 'ID', 'movie_name': 'Title', 'year': 'Year', 'directors': 'Directors', \n",
    "                                    'actors': 'Actors', 'critic_rating': 'Content_Rating', 'genre': 'Genre',\n",
    "                                    'duration': 'Duration'})\n",
    "\n",
    "new_tableB = tableB.rename(columns={'id': 'ID', 'movie_name': 'Title', 'year': 'Year', 'directors': 'Directors', \n",
    "                                    'actors': 'Actors', 'movie_rating': 'Content_Rating', 'genre': 'Genre',\n",
    "                                    'duration': 'Duration'})\n",
    "\n",
    "print('\\n The new columns are: \\n', new_tableA.columns, '\\n', new_tableB.columns)\n",
    "\n",
    "# store new_tableA and new_tableB in the cleaned directory\n",
    "save_dir = 'cleaned/movie/Roger_Ebert-IMDB'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "new_tableA.to_csv(os.path.join(save_dir, 'tableA.csv'), index=False)\n",
    "new_tableB.to_csv(os.path.join(save_dir, 'tableB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7530406d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp_tableA = new_tableA[['ID', 'Directors', 'Title', 'Year']].copy()\n",
    "temp_tableB = new_tableB[['ID', 'Directors', 'Title', 'Year']].copy()\n",
    "\n",
    "temp_tableA['key'] = 1\n",
    "temp_tableB['key'] = 1\n",
    "\n",
    "cross_joined_df = pd.merge(temp_tableA, temp_tableB, on='key', suffixes=('_A', '_B')).drop('key', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1866bb15",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_similarity(title_a, title_b):\n",
    "    set_a = set(title_a.lower().split())\n",
    "    set_b = set(title_b.lower().split())\n",
    "    intersection = set_a.intersection(set_b)\n",
    "    smaller_set_size = min(len(set_a), len(set_b))\n",
    "    if smaller_set_size > 0:\n",
    "        return len(intersection) / smaller_set_size\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "cross_joined_df['similarity'] = cross_joined_df.apply(lambda row: compute_similarity(row['Title_A'], row['Title_B']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "031a85de",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "high_sim_df = cross_joined_df[cross_joined_df['similarity']>0.7]\n",
    "\n",
    "high_sim_df = high_sim_df[(high_sim_df['Year_A']==high_sim_df['Year_B']) | (high_sim_df['Year_A'].isnull()) | (high_sim_df['Year_B'].isnull())]\n",
    "pos_candidates = pd.concat([high_sim_df[(high_sim_df['similarity']==1)],\n",
    "                            high_sim_df[high_sim_df['similarity']!=1]], axis=0)\n",
    "pos_candidates = pos_candidates[['ID_A', 'ID_B', 'Title_A', 'Title_B', 'Directors_A', 'Directors_B', 'Year_A', 'Year_B']]\n",
    "\n",
    "low_sim_df = cross_joined_df[cross_joined_df['similarity']<=0.7]\n",
    "neg_candidates = low_sim_df.sample(n=1000, random_state=SEED)\n",
    "neg_candidates = neg_candidates[['ID_A', 'ID_B', 'Title_A', 'Title_B', 'Directors_A', 'Directors_B', 'Year_A', 'Year_B']]\n",
    "\n",
    "pos_candidates.to_csv(os.path.join(save_dir, 'pos_candidates.csv'), index=False)\n",
    "neg_candidates.to_csv(os.path.join(save_dir, 'neg_candidates.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeddc45",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f33cc1bd",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Product Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ffd095d7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "src_product_dir_path = 'dirty/product'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03992d80",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Baby Products\n",
    "\n",
    "The dataset consist of tableA from Baby R US and tableB from Buy Buy Baby.\n",
    "\n",
    "Metadata of the source data: The Baby R US dataset comprises 3556 samples, while the Buy Buy Baby dataset contains 6913 samples. Human annotators have labeled 108 pairs as gold-standard matches. Additionally, there are 32 pairs that exhibit an exact match at the string level in the 'title' column.\n",
    "\n",
    "\n",
    "The following are details to generate postive candidates (matches) and negative candidates (non-matches):\n",
    "\n",
    "**postive candidates** : \n",
    "1. The titleA and titleB are significant similar (similarity larger than 0.85, determined by their overlapping size).\n",
    "\n",
    "**negative candidates** :\n",
    "1. The idA and idB is distinct from the positive candidates; AND\n",
    "3. 1000 pairs are randonly selected afterward.\n",
    "\n",
    "The ['Title', 'Company', 'Price'] columns are used for the later manual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "745c79f6",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'baby_products/csv_files'\n",
    "tableA_path = os.path.join(src_product_dir_path, dataset_path, 'babies_r_us.csv')\n",
    "tableB_path = os.path.join(src_product_dir_path, dataset_path, 'buy_buy_baby.csv')\n",
    "\n",
    "tableA = pd.read_csv(tableA_path)\n",
    "tableB = pd.read_csv(tableB_path)\n",
    "\n",
    "tableA.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "tableB.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "match_path = os.path.join(src_product_dir_path, dataset_path, 'labeled_data.csv')\n",
    "gold_match = pd.read_csv(match_path, skiprows=5)\n",
    "\n",
    "print(len(set(tableA.title).intersection(set(tableB.title))))\n",
    "print(gold_match.product_is_match.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "01689231",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old columns are: \n",
      " Index(['int_id', 'ext_id', 'title', 'SKU', 'price', 'is_discounted',\n",
      "       'category', 'company_struct', 'company_free', 'brand', 'weight',\n",
      "       'length', 'width', 'height', 'fabrics', 'colors', 'materials'],\n",
      "      dtype='object') \n",
      " Index(['int_id', 'ext_id', 'title', 'SKU', 'price', 'is_discounted',\n",
      "       'category', 'company_struct', 'company_free', 'brand', 'weight',\n",
      "       'length', 'width', 'height', 'fabrics', 'colors', 'materials'],\n",
      "      dtype='object')\n",
      "\n",
      " The new columns are: \n",
      " Index(['ID', 'Title', 'SKU', 'Price', 'Is_Discounted', 'Category', 'Company',\n",
      "       'Weight', 'Colors', 'Materials', 'Dimensions'],\n",
      "      dtype='object') \n",
      " Index(['ID', 'Title', 'SKU', 'Price', 'Is_Discounted', 'Category', 'Company',\n",
      "       'Weight', 'Colors', 'Materials', 'Dimensions'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# clean up\n",
    "print('The old columns are: \\n', tableA.columns, '\\n', tableB.columns)\n",
    "\n",
    "tableA['is_discounted'] = tableA['is_discounted'].astype(bool)\n",
    "tableA['dimensions'] = tableA.apply(lambda x: '/'.join([x[col] if not pd.isnull(x[col]) else '-' for col in ['length', 'width', 'height']]), axis=1)\n",
    "tableB['dimensions'] = tableB.apply(lambda x: '/'.join([x[col] if not pd.isnull(x[col]) else '-' for col in ['length', 'width', 'height']]), axis=1)\n",
    "\n",
    "new_tableA = tableA.drop(columns=['ext_id', 'company_free', 'brand', 'fabrics', 'length', 'width', 'height'])\\\n",
    "                   .rename(columns={'int_id': 'ID', 'title': 'Title', 'price': 'Price', \n",
    "                                    'is_discounted': 'Is_Discounted', 'category': 'Category',\n",
    "                                    'company_struct': 'Company', 'weight': 'Weight', 'colors': 'Colors', \n",
    "                                    'materials': 'Materials', 'dimensions': 'Dimensions'})\n",
    "\n",
    "new_tableB = tableB.drop(columns=['ext_id', 'company_free', 'brand', 'fabrics', 'length', 'width', 'height'])\\\n",
    "                   .rename(columns={'int_id': 'ID', 'title': 'Title', 'price': 'Price', \n",
    "                                    'is_discounted': 'Is_Discounted', 'category': 'Category',\n",
    "                                    'company_struct': 'Company', 'weight': 'Weight', 'colors': 'Colors', \n",
    "                                    'materials': 'Materials', 'dimensions': 'Dimensions'})\n",
    "\n",
    "print('\\n The new columns are: \\n', new_tableA.columns, '\\n', new_tableB.columns)\n",
    "\n",
    "# store new_tableA and new_tableB in the cleaned directory\n",
    "save_dir = 'cleaned/product/Baby_U_US-Buy_Buy_Baby'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "new_tableA.to_csv(os.path.join(save_dir, 'tableA.csv'), index=False)\n",
    "new_tableB.to_csv(os.path.join(save_dir, 'tableB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6ddb2e14",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp_tableA = new_tableA[['ID', 'Title', 'Company', 'Price']].copy()\n",
    "temp_tableB = new_tableB[['ID', 'Title', 'Company', 'Price']].copy()\n",
    "\n",
    "temp_tableA['temp_key'] = 1\n",
    "temp_tableB['temp_key'] = 1\n",
    "\n",
    "cross_joined_df = pd.merge(temp_tableA, temp_tableB, on='temp_key', suffixes=('_A', '_B')).drop('key', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "04e1b2dd",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_similarity(title_a, title_b):\n",
    "    set_a = set(title_a.lower().split())\n",
    "    set_b = set(title_b.lower().split())\n",
    "    intersection = set_a.intersection(set_b)\n",
    "    smaller_set_size = min(len(set_a), len(set_b))\n",
    "    if smaller_set_size > 0:\n",
    "        return len(intersection) / smaller_set_size\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "cross_joined_df['similarity'] = cross_joined_df.apply(lambda row: compute_similarity(row['Title_A'], row['Title_B']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6af09f98",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "high_sim_df = cross_joined_df[cross_joined_df['similarity']>0.85]\n",
    "pos_candidates = pd.concat([high_sim_df[(high_sim_df['similarity']==1)],\n",
    "                            high_sim_df[high_sim_df['similarity']!=1].sample(n=900, random_state=SEED)], axis=0)\n",
    "pos_candidates = pos_candidates[['ID_A', 'ID_B', 'Title_A', 'Title_B', 'Company_A', 'Company_B', 'Price_A', 'Price_B']]\n",
    "\n",
    "low_sim_df = cross_joined_df[cross_joined_df['similarity']<=0.85]\n",
    "neg_candidates = low_sim_df.sample(n=1000, random_state=SEED)\n",
    "neg_candidates = neg_candidates[['ID_A', 'ID_B', 'Title_A', 'Title_B', 'Company_A', 'Company_B', 'Price_A', 'Price_B']]\n",
    "\n",
    "pos_candidates.to_csv(os.path.join(save_dir, 'pos_candidates.csv'), index=False)\n",
    "neg_candidates.to_csv(os.path.join(save_dir, 'neg_candidates.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab65a8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ad4a41f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Cosmetics\n",
    "\n",
    "The dataset consist of tableA from Amazon and tableB from Sephora.\n",
    "\n",
    "Metadata of the source data: The Amazon dataset comprises 6443 samples, while the Sephora dataset contains 11026 samples. There is only 1 pair that exhibit an exact match at the string level in the 'description' column, and no golden labels are provided.\n",
    "\n",
    "\n",
    "The following are details to generate postive candidates (matches) and negative candidates (non-matches):\n",
    "\n",
    "**postive candidates** : \n",
    "1. The descriptionA contains descriptionB, vice verse; AND\n",
    "2. The colorA and colorB are the same (if existing).\n",
    "3. 1100 pairs are randonly selected afterward.\n",
    "\n",
    "**negative candidates** :\n",
    "1. The idA and idB is distinct from the positive candidates; AND\n",
    "2. 1000 pairs are randonly selected afterward.\n",
    "\n",
    "The ['Description', 'Color', 'Price'] columns are used for the later manual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e4351d61",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'cosmetics/csv_files'\n",
    "tableA_path = os.path.join(src_product_dir_path, dataset_path, 'amazon.csv')\n",
    "tableB_path = os.path.join(src_product_dir_path, dataset_path, 'sephora.csv')\n",
    "\n",
    "tableA = pd.read_csv(tableA_path, encoding='latin1')\n",
    "tableB = pd.read_csv(tableB_path)\n",
    "\n",
    "tableA.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "tableB.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "tableA\n",
    "print(len(set(tableA.Description).intersection(set(tableB.Description))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7ce49d23",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old columns are: \n",
      " Index(['ID', 'Price', 'Color', 'Description'], dtype='object') \n",
      " Index(['Product_id', 'Description', 'Price', 'Color'], dtype='object')\n",
      "\n",
      " The new columns are: \n",
      " Index(['ID', 'Description', 'Price', 'Color'], dtype='object') \n",
      " Index(['ID', 'Description', 'Price', 'Color'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# clean up\n",
    "print('The old columns are: \\n', tableA.columns, '\\n', tableB.columns)\n",
    "\n",
    "new_tableA = tableA\n",
    "new_tableB = tableB.rename(columns={'Product_id': 'ID'})\n",
    "new_tableA = new_tableA[['ID', 'Description', 'Price', 'Color']]\n",
    "new_tableB = new_tableB[['ID', 'Description', 'Price', 'Color']]\n",
    "\n",
    "print('\\n The new columns are: \\n', new_tableA.columns, '\\n', new_tableB.columns)\n",
    "\n",
    "# store new_tableA and new_tableB in the cleaned directory\n",
    "save_dir = 'cleaned/product/Amazon_Sephora'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "new_tableA.to_csv(os.path.join(save_dir, 'tableA.csv'), index=False)\n",
    "new_tableB.to_csv(os.path.join(save_dir, 'tableB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3016d237",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0cdf81a9734d26be391a5b76731f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_tableA = new_tableA[['ID', 'Description', 'Color', 'Price']].copy()\n",
    "temp_tableB = new_tableB[['ID', 'Description', 'Color', 'Price']].copy()\n",
    "\n",
    "connection = duckdb.connect(database=':memory:', read_only=False)\n",
    "connection.register('A', temp_tableA)\n",
    "connection.register('B', temp_tableB)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    ta.ID AS ID_A,\n",
    "    tb.ID AS ID_B,\n",
    "    ta.Description AS Description_A,\n",
    "    tb.Description AS Description_B,\n",
    "    ta.Color AS Color_A,\n",
    "    tb.Color AS Color_B,\n",
    "    ta.Price AS Price_A,\n",
    "    tb.Price AS Price_B, \n",
    "    \n",
    "FROM\n",
    "    A ta\n",
    "CROSS JOIN\n",
    "    B tb\n",
    "WHERE ((LOWER(ta.Description) LIKE '%' || LOWER(tb.Description) || '%') OR \n",
    "    (LOWER(tb.Description) LIKE '%' || LOWER(ta.Description) || '%'))\n",
    "    AND ((ta.Color IS NULL OR tb.Color IS NULL) OR (ta.Color == tb.Color))  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "result = connection.execute(query).fetchdf()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b74d0d87",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## generate pos_candidates & neg_candidates, and store them in the cleaned directory\n",
    "pos_candidates = pd.DataFrame({\n",
    "    'ID_A': result['ID_A'], 'ID_B': result['ID_B'],\n",
    "    'Description_A': result['Description_A'], 'Description_B': result['Description_B'],\n",
    "    'Color_A': result['Color_A'], 'Color_B': result['Color_B'],\n",
    "    'Price_A': result['Price_A'], 'Price_B': result['Price_B'],\n",
    "})\n",
    "\n",
    "temp_tableA['temp_key'] = 1\n",
    "temp_tableB['temp_key'] = 1\n",
    "all_id_pairs = pd.merge(temp_tableA[['ID', 'temp_key']], temp_tableB[['ID', 'temp_key']], on='temp_key', suffixes=('_A', '_B'))\n",
    "all_id_pairs = all_id_pairs.drop('temp_key', axis=1)\n",
    "\n",
    "\n",
    "exclude_pairs = pos_candidates[['ID_A', 'ID_B']]\n",
    "neg_candidates = pd.concat([all_id_pairs, exclude_pairs], axis=0).drop_duplicates(keep=False)\n",
    "\n",
    "pos_candidates = pos_candidates.sample(n=1100, random_state=SEED)\n",
    "neg_candidates = neg_candidates.sample(n=1000, random_state=SEED)\n",
    "\n",
    "neg_candidates = neg_candidates.merge(temp_tableA[['ID', 'Description', 'Color', 'Price']], \n",
    "                                      left_on='ID_A', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates.merge(temp_tableB[['ID', 'Description', 'Color', 'Price']], \n",
    "                                      left_on='ID_B', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates[list(pos_candidates.columns)]\n",
    "\n",
    "pos_candidates.to_csv(os.path.join(save_dir, 'pos_candidates.csv'), index=False)\n",
    "neg_candidates.to_csv(os.path.join(save_dir, 'neg_candidates.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d4d66",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "517c53b0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Products\n",
    "\n",
    "The dataset consist of tableA from Walmart and tableB from Amazon.\n",
    "\n",
    "Metadata of the source data: The Walmart dataset comprises 2554 samples, while the Amazon dataset contains 22074 samples. Human annotators have labeled 1149 pairs as gold-standard matches. Additionally, there are 35 pairs that exhibit an exact match at the string level in the 'title' column. After several failure attempt, we decide to continue using the golden labeled samples.\n",
    "\n",
    "\n",
    "The following are details to generate postive candidates (matches) and negative candidates (non-matches):\n",
    "\n",
    "**postive candidates** : \n",
    "1. 1100 paris are randomly selected from the golden matched pairs.\n",
    "\n",
    "**negative candidates** :\n",
    "1. The idA and idB is distinct from the golden matched pairs; AND\n",
    "3. 1000 pairs are randomly selected afterward.\n",
    "\n",
    "The ['Title', 'Model_Number', 'Brand', 'Price'] columns are used for the later manual check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "fa894667",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "(1154, 2)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'products'\n",
    "tableA_path = os.path.join(src_product_dir_path, dataset_path, 'walmart.csv')\n",
    "tableB_path = os.path.join(src_product_dir_path, dataset_path, 'amazon.csv')\n",
    "\n",
    "tableA = pd.read_csv(tableA_path)\n",
    "tableB = pd.read_csv(tableB_path)\n",
    "\n",
    "tableA.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "tableB.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
    "\n",
    "match_path = os.path.join(src_product_dir_path, dataset_path, 'matches_walmart_amazon.csv')\n",
    "gold_match = pd.read_csv(match_path)\n",
    "\n",
    "print(len(set(tableA.title).intersection(set(tableB.title))))\n",
    "print(gold_match.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "95c0f554",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old columns are: \n",
      " Index(['custom_id', 'id', 'upc', 'brand', 'groupname', 'title', 'price',\n",
      "       'shelfdescr', 'shortdescr', 'longdescr', 'imageurl', 'orig_shelfdescr',\n",
      "       'orig_shortdescr', 'orig_longdescr', 'modelno', 'shipweight',\n",
      "       'dimensions'],\n",
      "      dtype='object') \n",
      " Index(['custom_id', 'url', 'asin', 'brand', 'modelno', 'category1',\n",
      "       'pcategory1', 'category2', 'pcategory2', 'title', 'listprice', 'price',\n",
      "       'prodfeatures', 'techdetails', 'proddescrshort', 'proddescrlong',\n",
      "       'dimensions', 'imageurl', 'itemweight', 'shipweight',\n",
      "       'orig_prodfeatures', 'orig_techdetails'],\n",
      "      dtype='object')\n",
      "\n",
      " The new columns are: \n",
      " Index(['ID', 'Brand', 'Title', 'Price', 'Description', 'Model_Number',\n",
      "       'Ship_Weight', 'Dimensions'],\n",
      "      dtype='object') \n",
      " Index(['ID', 'Brand', 'Title', 'Price', 'Description', 'Model_Number',\n",
      "       'Ship_Weight', 'Dimensions'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# clean up\n",
    "print('The old columns are: \\n', tableA.columns, '\\n', tableB.columns)\n",
    "\n",
    "new_tableA = tableA.drop(columns=['id', 'upc', 'groupname', 'shelfdescr', 'longdescr', 'imageurl', \n",
    "                                  'orig_shelfdescr', 'orig_shortdescr', 'orig_longdescr'])\\\n",
    "                   .rename(columns={'custom_id': 'ID', 'brand': 'Brand', 'title': 'Title', \n",
    "                                    'price': 'Price', 'shortdescr': 'Description', 'modelno': 'Model_Number',\n",
    "                                    'shipweight': 'Ship_Weight', 'dimensions': 'Dimensions'})\n",
    "new_tableB = tableB.drop(columns=['url', 'asin', 'category1', 'pcategory1', 'category2', 'pcategory2',\n",
    "                                  'listprice', 'prodfeatures', 'techdetails', 'proddescrlong', 'imageurl', \n",
    "                                  'itemweight', 'orig_prodfeatures', 'orig_techdetails'])\\\n",
    "                   .rename(columns={'custom_id': 'ID', 'brand': 'Brand', 'title': 'Title', \n",
    "                                    'price': 'Price', 'proddescrshort': 'Description', 'modelno': 'Model_Number',\n",
    "                                    'shipweight': 'Ship_Weight', 'dimensions': 'Dimensions'})\n",
    "new_tableA = new_tableA[['ID', 'Brand', 'Title', 'Price', 'Description', 'Model_Number', 'Ship_Weight', 'Dimensions']]\n",
    "new_tableB = new_tableB[['ID', 'Brand', 'Title', 'Price', 'Description', 'Model_Number', 'Ship_Weight', 'Dimensions']]\n",
    "\n",
    "print('\\n The new columns are: \\n', new_tableA.columns, '\\n', new_tableB.columns)\n",
    "\n",
    "# store new_tableA and new_tableB in the cleaned directory\n",
    "save_dir = 'cleaned/product/Walmart-Amazon'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "new_tableA.to_csv(os.path.join(save_dir, 'tableA.csv'), index=False)\n",
    "new_tableB.to_csv(os.path.join(save_dir, 'tableB.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ce5a1769",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "temp_tableA = new_tableA[['ID', 'Title', 'Model_Number', 'Brand', 'Price']].copy()\n",
    "temp_tableB = new_tableB[['ID', 'Title', 'Model_Number', 'Brand', 'Price']].copy()\n",
    "\n",
    "temp_tableA['temp_key'] = 1\n",
    "temp_tableB['temp_key'] = 1\n",
    "\n",
    "all_id_pairs = pd.merge(temp_tableA[['ID', 'temp_key']], temp_tableB[['ID', 'temp_key']], on='temp_key', suffixes=('_A', '_B'))\n",
    "all_id_pairs = all_id_pairs.drop('temp_key', axis=1)\n",
    "\n",
    "pos_candidates = gold_match[['id1', 'id2']]\n",
    "pos_candidates = pos_candidates.rename(columns={'id1': 'ID_A', 'id2': 'ID_B'})\n",
    "\n",
    "\n",
    "exclude_pairs = pos_candidates[['ID_A', 'ID_B']]\n",
    "neg_candidates = pd.concat([all_id_pairs, exclude_pairs], axis=0).drop_duplicates(keep=False)\n",
    "\n",
    "pos_candidates = pos_candidates.sample(n=1100, random_state=SEED)\n",
    "neg_candidates = neg_candidates.sample(n=1000, random_state=SEED)\n",
    "\n",
    "pos_candidates = pos_candidates.merge(temp_tableA[['ID', 'Title', 'Model_Number', 'Brand', 'Price']], \n",
    "                                      left_on='ID_A', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "pos_candidates = pos_candidates.merge(temp_tableB[['ID', 'Title', 'Model_Number', 'Brand', 'Price']], \n",
    "                                      left_on='ID_B', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "pos_candidates = pos_candidates[['ID_A', 'ID_B', 'Title_A', 'Title_B', 'Model_Number_A', 'Model_Number_B',\n",
    "                                 'Brand_A', 'Brand_B', 'Price_A', 'Price_B']]\n",
    "\n",
    "neg_candidates = neg_candidates.merge(temp_tableA[['ID', 'Title', 'Model_Number', 'Brand', 'Price']], \n",
    "                                      left_on='ID_A', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates.merge(temp_tableB[['ID', 'Title', 'Model_Number', 'Brand', 'Price']], \n",
    "                                      left_on='ID_B', right_on='ID', how='left', suffixes=('_A', '_B')).drop(columns=['ID'])\n",
    "neg_candidates = neg_candidates[list(pos_candidates.columns)]\n",
    "\n",
    "pos_candidates.to_csv(os.path.join(save_dir, 'pos_candidates.csv'), index=False)\n",
    "neg_candidates.to_csv(os.path.join(save_dir, 'neg_candidates.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766aafce",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45cadb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "049f2894",
   "metadata": {},
   "source": [
    "# Manually Clean-UP & Create the Final Version\n",
    "\n",
    "Upon completing the previous steps, we have created both positive and negative candidate pairs for the entity matching task. To ensure the accuracy of the new benchmark, we will perform a thorough manual review of all pairs. Once this step was finished, we can produce the final version of the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2afb8e",
   "metadata": {},
   "source": [
    "## Create the manual label column\n",
    "\n",
    "For all neg_candidates and pos_candidates files, we will introduce a new Flag column to label the uncertain and mislabeled pairs. You can tailor your own requirement to filter out pairs you don't want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3d5a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_dirs = {\n",
    "    'restaurant': ['cleaned/restaurant/Fodors-Zagats', \n",
    "                   'cleaned/restaurant/Zomato-Yelp', \n",
    "                   'cleaned/restaurant/Yellow_Pages-Yelp'],\n",
    "    \n",
    "    'book': ['cleaned/book/Amazon-Barnes_and_Noble', \n",
    "             'cleaned/book/Barnes_and_Noble-Half', \n",
    "             'cleaned/book/Goodreads-Barnes_and_Noble'],\n",
    "    \n",
    "    'paper': ['cleaned/paper/Arxiv-Neurips', \n",
    "              'cleaned/paper/Arxiv-THECVF', \n",
    "              'cleaned/paper/Google_Scholar-DBLP'],\n",
    "    \n",
    "    'movie': ['cleaned/movie/Amazon-Rotten_Tomatoes', \n",
    "              'cleaned/movie/Rotten_Tomatoes-IMDB', \n",
    "              'cleaned/movie/Roger_Ebert-IMDB'],\n",
    "    \n",
    "    'product': ['cleaned/product/Amazon_Sephora', \n",
    "                'cleaned/product/Baby_U_US-Buy_Buy_Baby', \n",
    "                'cleaned/product/Walmart-Amazon'],\n",
    "}\n",
    "\n",
    "for domain in cleaned_data_dirs:\n",
    "    for data_dir in cleaned_data_dirs[domain]:\n",
    "        pos_candidates = pd.read_csv(os.path.join(data_dir, 'pos_candidates.csv'))\n",
    "        neg_candidates = pd.read_csv(os.path.join(data_dir, 'neg_candidates.csv'))\n",
    "        \n",
    "        pos_candidates.insert(2, 'Flag', pd.NA)\n",
    "        if not os.path.exists(os.path.join(data_dir, 'pos_candidates_manual.csv')):\n",
    "            pos_candidates.to_csv(os.path.join(data_dir, 'pos_candidates_manual.csv'), index=False)\n",
    "\n",
    "        neg_candidates.insert(2, 'Flag', pd.NA)\n",
    "        if not os.path.exists(os.path.join(data_dir, 'neg_candidates_manual.csv')):\n",
    "            neg_candidates.to_csv(os.path.join(data_dir, 'neg_candidates_manual.csv'), index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2eac8038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abe1a85e",
   "metadata": {},
   "source": [
    "## Create Final Datasets\n",
    "\n",
    "To finalize the benchmark, we will first filter out data points that are manually identified as incorrect, thereby creating the golden positive and negative pairs. Subsequently, all pairs will be consolidated to form the final dataset, which will adhere to the following rules:\n",
    "\n",
    "1. The positive pairs can consist of no more than 500 records;\n",
    "2. Each dataset will comprise a total of 1000 records.\n",
    "\n",
    "To facilitate downstream applications, we will merge Table A and Table B into a single file. Each entry will be distinguished by a suffix: '_A' for entries from Table A and '_B' for entries from Table B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38a48a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_dirs = {\n",
    "    'restaurant': ['cleaned/restaurant/Fodors-Zagats', \n",
    "                   'cleaned/restaurant/Zomato-Yelp', \n",
    "                   'cleaned/restaurant/Yellow_Pages-Yelp'],\n",
    "    \n",
    "    'book': ['cleaned/book/Amazon-Barnes_and_Noble', \n",
    "             'cleaned/book/Barnes_and_Noble-Half', \n",
    "             'cleaned/book/Goodreads-Barnes_and_Noble'],\n",
    "    \n",
    "    'paper': ['cleaned/paper/Arxiv-Neurips', \n",
    "              'cleaned/paper/Arxiv-THECVF', \n",
    "              'cleaned/paper/Google_Scholar-DBLP'],\n",
    "    \n",
    "    'movie': ['cleaned/movie/Amazon-Rotten_Tomatoes', \n",
    "              'cleaned/movie/Rotten_Tomatoes-IMDB', \n",
    "              'cleaned/movie/Roger_Ebert-IMDB'],\n",
    "    \n",
    "    'product': ['cleaned/product/Amazon_Sephora', \n",
    "                'cleaned/product/Baby_U_US-Buy_Buy_Baby', \n",
    "                'cleaned/product/Walmart-Amazon'],\n",
    "}\n",
    "\n",
    "for domain in cleaned_data_dirs:\n",
    "    for data_dir in cleaned_data_dirs[domain]:\n",
    "        tableA = pd.read_csv(os.path.join(data_dir, 'tableA.csv'))\n",
    "        tableB = pd.read_csv(os.path.join(data_dir, 'tableB.csv'))\n",
    "        pos_candidates = pd.read_csv(os.path.join(data_dir, 'pos_candidates_manual.csv'))\n",
    "        neg_candidates = pd.read_csv(os.path.join(data_dir, 'neg_candidates_manual.csv'))\n",
    "        \n",
    "        pos_pairs = pos_candidates[pos_candidates.Flag.isnull()]\n",
    "        neg_pairs = neg_candidates[neg_candidates.Flag.isnull()]\n",
    "        \n",
    "        pos_num = min(len(pos_pairs), 500)\n",
    "        neg_num = 1000 - pos_num\n",
    "        \n",
    "        pos_pairs = pos_pairs.sample(pos_num, random_state=SEED)\n",
    "        neg_pairs = neg_pairs.sample(neg_num, random_state=SEED)\n",
    "        \n",
    "        pos_pairs = pos_pairs[['ID_A', 'ID_B']]\n",
    "        neg_pairs = neg_pairs[['ID_A', 'ID_B']]\n",
    "        \n",
    "        pairs_id = pd.concat([pos_pairs, neg_pairs], axis=0)\n",
    "        pairs_with_tableA = pairs_id.merge(tableA, left_on='ID_A', right_on='ID', how='left')\n",
    "        pairs_with_tableA = pairs_with_tableA.drop(columns=['ID_A', 'ID_B', 'ID']).rename(columns=lambda x: x + '_A')\n",
    "        pairs_with_tableB = pairs_id.merge(tableB, left_on='ID_B', right_on='ID', how='left')\n",
    "        pairs_with_tableB = pairs_with_tableB.drop(columns=['ID_A', 'ID_B', 'ID']).rename(columns=lambda x: x + '_B')\n",
    "        \n",
    "        pairs = pd.concat([pairs_with_tableA, pairs_with_tableB], axis=1)\n",
    "        pairs['Gold'] = [1] * pos_num + [0] * neg_num\n",
    "        \n",
    "        save_path =  '/'.join(['final'] + data_dir.split('/')[1:] + ['labeled_paris.csv'])\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(os.path.dirname(save_path))\n",
    "        pairs.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c67a1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
